{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMI Gesture Recognition Training Notebook\n",
    "\n",
    "This notebook trains the four-branch gesture recognition model for the CMI competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from feature_processor import FeatureProcessor\n",
    "from src.model import create_model\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "print(\"Loading training data...\")\n",
    "train_df = pl.read_csv(\"../dataset/train.csv\")\n",
    "demographics_df = pl.read_csv(\"../dataset/train_demographics.csv\")\n",
    "\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(f\"Demographics data shape: {demographics_df.shape}\")\n",
    "print(f\"\\nColumns in training data: {len(train_df.columns)}\")\n",
    "print(f\"Unique sequences: {train_df['sequence_id'].n_unique()}\")\n",
    "print(f\"Unique gestures: {train_df['gesture'].n_unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types and missing values\n",
    "print(\"Gesture distribution:\")\n",
    "gesture_counts = (\n",
    "    train_df.group_by(\"gesture\")\n",
    "    .agg(pl.count().alias(\"count\"))\n",
    "    .sort(\"count\", descending=True)\n",
    ")\n",
    "print(gesture_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define gesture classes\n",
    "target_gestures = [\n",
    "    \"Above ear - pull hair\",\n",
    "    \"Cheek - pinch skin\",\n",
    "    \"Eyebrow - pull hair\",\n",
    "    \"Eyelash - pull hair\",\n",
    "    \"Forehead - pull hairline\",\n",
    "    \"Forehead - scratch\",\n",
    "    \"Neck - pinch skin\",\n",
    "    \"Neck - scratch\",\n",
    "]\n",
    "\n",
    "non_target_gestures = [\n",
    "    \"Text on phone\",\n",
    "    \"Wave hello\",\n",
    "    \"Write name in air\",\n",
    "    \"Pull air toward your face\",\n",
    "    \"Feel around in tray and pull out an object\",\n",
    "    \"Glasses on/off\",\n",
    "    \"Drink from bottle/cup\",\n",
    "    \"Scratch knee/leg skin\",\n",
    "    \"Write name on leg\",\n",
    "    \"Pinch knee/leg skin\",\n",
    "]\n",
    "\n",
    "all_gestures = target_gestures + non_target_gestures\n",
    "print(f\"Total gesture classes: {len(all_gestures)}\")\n",
    "\n",
    "# Create label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "train_df = train_df.with_columns(\n",
    "    pl.Series(label_encoder.fit_transform(train_df[\"gesture\"].to_numpy())).alias(\n",
    "        \"gesture_id\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"\\nGesture ID mapping:\")\n",
    "for i, gesture in enumerate(label_encoder.classes_):\n",
    "    print(f\"{i}: {gesture}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing and Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature columns\n",
    "acc_cols = [\"acc_x\", \"acc_y\", \"acc_z\"]\n",
    "rot_cols = [\"rot_w\", \"rot_x\", \"rot_y\", \"rot_z\"]\n",
    "thm_cols = [f\"thm_{i}\" for i in range(1, 6)]\n",
    "tof_cols = [f\"tof_{i}_v{j}\" for i in range(1, 6) for j in range(64)]\n",
    "\n",
    "print(f\"Accelerometer columns: {len(acc_cols)}\")\n",
    "print(f\"Rotation columns: {len(rot_cols)}\")\n",
    "print(f\"Thermal columns: {len(thm_cols)}\")\n",
    "print(f\"ToF columns: {len(tof_cols)}\")\n",
    "print(f\"Total sensor columns: {len(acc_cols + rot_cols + thm_cols + tof_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare sequences with enhanced features\n",
    "print(\"Preparing sequences with enhanced feature processing...\")\n",
    "sequences = []\n",
    "sequence_lengths = []\n",
    "\n",
    "feature_processor = FeatureProcessor()\n",
    "\n",
    "# Group by sequence_id and process each sequence\n",
    "grouped = train_df.group_by(\"sequence_id\")\n",
    "\n",
    "for seq_id, group in tqdm(grouped, desc=\"Processing sequences\"):\n",
    "    try:\n",
    "        # Get sequence data and metadata\n",
    "        gesture_id = group[\"gesture_id\"][0]\n",
    "\n",
    "        # Create enhanced features using FeatureProcessor\n",
    "        enhanced_features = feature_processor.create_sequence_features(group)\n",
    "\n",
    "        # Store both original and enhanced data\n",
    "        sequences.append(\n",
    "            {\n",
    "                \"sequence_id\": seq_id[0],\n",
    "                \"enhanced_data\": enhanced_features,\n",
    "                \"label\": gesture_id,\n",
    "            },\n",
    "        )\n",
    "        sequence_lengths.append(enhanced_features[\"sequence_length\"])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing sequence {seq_id[0]}: {e}\")\n",
    "        # Fallback to original processing\n",
    "        seq_data = group.select(acc_cols + rot_cols + thm_cols + tof_cols).to_numpy()\n",
    "        sequences.append(\n",
    "            {\n",
    "                \"sequence_id\": seq_id[0],\n",
    "                \"data\": seq_data,\n",
    "                \"label\": gesture_id,\n",
    "            },\n",
    "        )\n",
    "        sequence_lengths.append(len(seq_data))\n",
    "\n",
    "print(f\"\\nProcessed {len(sequences)} sequences\")\n",
    "print(\"Sequence length statistics:\")\n",
    "print(f\"  Min: {min(sequence_lengths)}\")\n",
    "print(f\"  Max: {max(sequence_lengths)}\")\n",
    "print(f\"  Mean: {np.mean(sequence_lengths):.1f}\")\n",
    "print(f\"  Median: {np.median(sequence_lengths):.1f}\")\n",
    "\n",
    "# Check enhanced feature dimensions\n",
    "if \"enhanced_data\" in sequences[0]:\n",
    "    sample_features = sequences[0][\"enhanced_data\"]\n",
    "    print(\"\\nEnhanced feature dimensions:\")\n",
    "    print(f\"  ToF: {sample_features['tof'].shape[1]} features\")\n",
    "    print(f\"  ACC: {sample_features['acc'].shape[1]} features (enhanced)\")\n",
    "    print(f\"  ROT: {sample_features['rot'].shape[1]} features (enhanced)\")\n",
    "    print(f\"  THM: {sample_features['thm'].shape[1]} features (enhanced)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose max sequence length (use 95th percentile to avoid extreme outliers)\n",
    "max_seq_length = int(np.percentile(sequence_lengths, 95))\n",
    "print(f\"Using max sequence length: {max_seq_length}\")\n",
    "\n",
    "# Split sequences by gesture for stratified split\n",
    "sequence_ids = [seq[\"sequence_id\"] for seq in sequences]\n",
    "labels = [seq[\"label\"] for seq in sequences]\n",
    "\n",
    "# Stratified train/validation split\n",
    "train_indices, val_indices = train_test_split(\n",
    "    range(len(sequences)),\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=labels,\n",
    ")\n",
    "\n",
    "train_sequences = [sequences[i] for i in train_indices]\n",
    "val_sequences = [sequences[i] for i in val_indices]\n",
    "\n",
    "print(f\"Training sequences: {len(train_sequences)}\")\n",
    "print(f\"Validation sequences: {len(val_sequences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CMIDataset(Dataset):\n",
    "    def __init__(self, sequences, max_length=None, use_enhanced_features=True):\n",
    "        self.sequences = sequences\n",
    "        self.max_length = max_length or max(\n",
    "            len(seq[\"enhanced_data\"][\"tof\"])\n",
    "            if \"enhanced_data\" in seq\n",
    "            else len(seq[\"data\"])\n",
    "            for seq in sequences\n",
    "        )\n",
    "        self.use_enhanced_features = use_enhanced_features\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        label = sequence[\"label\"]\n",
    "\n",
    "        if self.use_enhanced_features and \"enhanced_data\" in sequence:\n",
    "            # Use enhanced features\n",
    "            tof_data = sequence[\"enhanced_data\"][\"tof\"]\n",
    "            acc_data = sequence[\"enhanced_data\"][\"acc\"]\n",
    "            rot_data = sequence[\"enhanced_data\"][\"rot\"]\n",
    "            thm_data = sequence[\"enhanced_data\"][\"thm\"]\n",
    "        else:\n",
    "            # Use original features\n",
    "            data = sequence[\"data\"]\n",
    "\n",
    "            # Pad or truncate sequence\n",
    "            seq_len = len(data)\n",
    "            if seq_len < self.max_length:\n",
    "                # Pad with zeros\n",
    "                padding = np.zeros((self.max_length - seq_len, data.shape[1]))\n",
    "                data = np.vstack([data, padding])\n",
    "            elif seq_len > self.max_length:\n",
    "                # Truncate\n",
    "                data = data[: self.max_length]\n",
    "\n",
    "            # Split into sensor modalities\n",
    "            tof_data = data[:, :320]  # ToF features (320)\n",
    "            acc_data = data[:, 320:323]  # Accelerometer (3)\n",
    "            rot_data = data[:, 323:327]  # Rotation (4)\n",
    "            thm_data = data[:, 327:332]  # Thermal (5)\n",
    "\n",
    "            # Handle missing values (-1.0) by replacing with 0\n",
    "            tof_data = np.where(tof_data == -1.0, 0.0, tof_data)\n",
    "            acc_data = np.where(acc_data == -1.0, 0.0, acc_data)\n",
    "            rot_data = np.where(rot_data == -1.0, 0.0, rot_data)\n",
    "            thm_data = np.where(thm_data == -1.0, 0.0, thm_data)\n",
    "\n",
    "        # Ensure consistent length for enhanced features\n",
    "        if self.use_enhanced_features:\n",
    "            seq_len = len(tof_data)\n",
    "            if seq_len < self.max_length:\n",
    "                # Pad enhanced features\n",
    "                tof_padding = np.zeros((self.max_length - seq_len, tof_data.shape[1]))\n",
    "                acc_padding = np.zeros((self.max_length - seq_len, acc_data.shape[1]))\n",
    "                rot_padding = np.zeros((self.max_length - seq_len, rot_data.shape[1]))\n",
    "                thm_padding = np.zeros((self.max_length - seq_len, thm_data.shape[1]))\n",
    "\n",
    "                tof_data = np.vstack([tof_data, tof_padding])\n",
    "                acc_data = np.vstack([acc_data, acc_padding])\n",
    "                rot_data = np.vstack([rot_data, rot_padding])\n",
    "                thm_data = np.vstack([thm_data, thm_padding])\n",
    "            elif seq_len > self.max_length:\n",
    "                # Truncate\n",
    "                tof_data = tof_data[: self.max_length]\n",
    "                acc_data = acc_data[: self.max_length]\n",
    "                rot_data = rot_data[: self.max_length]\n",
    "                thm_data = thm_data[: self.max_length]\n",
    "\n",
    "        return {\n",
    "            \"tof\": torch.FloatTensor(tof_data),\n",
    "            \"acc\": torch.FloatTensor(acc_data),\n",
    "            \"rot\": torch.FloatTensor(rot_data),\n",
    "            \"thm\": torch.FloatTensor(thm_data),\n",
    "            \"label\": torch.LongTensor([label])[0],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "CONFIG = {\n",
    "    \"batch_size\": 16,\n",
    "    \"d_model\": 128,\n",
    "    \"num_heads\": 8,\n",
    "    \"max_seq_length\": max_seq_length,\n",
    "    \"use_enhanced_features\": True,\n",
    "    \"lr\": 1e-4,\n",
    "    \"weight_decay\": 1e-2,\n",
    "    \"patience\": 5,\n",
    "    \"decay_factor\": 0.5,\n",
    "    \"num_epochs\": 10,\n",
    "    \"device\": device,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets and dataloaders\n",
    "train_dataset = CMIDataset(\n",
    "    train_sequences,\n",
    "    max_length=CONFIG[\"max_seq_length\"],\n",
    "    use_enhanced_features=CONFIG[\"use_enhanced_features\"],\n",
    ")\n",
    "val_dataset = CMIDataset(\n",
    "    val_sequences,\n",
    "    max_length=CONFIG[\"max_seq_length\"],\n",
    "    use_enhanced_features=CONFIG[\"use_enhanced_features\"],\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CONFIG[\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=CONFIG[\"batch_size\"],\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = create_model(\n",
    "    d_model=CONFIG[\"d_model\"],\n",
    "    num_heads=CONFIG[\"num_heads\"],\n",
    "    seq_len=CONFIG[\"max_seq_length\"],\n",
    ").to(CONFIG[\"device\"])\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "print(f\"Trainable parameters: {trainable_params}\")\n",
    "print(f\"model at {CONFIG['device']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=CONFIG[\"lr\"],\n",
    "    weight_decay=CONFIG[\"weight_decay\"],\n",
    ")\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode=\"max\",\n",
    "    factor=CONFIG[\"decay_factor\"],\n",
    "    patience=CONFIG[\"patience\"],\n",
    ")\n",
    "\n",
    "num_epochs = CONFIG[\"num_epochs\"]\n",
    "best_val_acc = 0.0\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "print(f\"Training setup complete. Starting training for {num_epochs} epochs...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "\n",
    "    train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
    "    for batch in train_pbar:\n",
    "        # Move to device\n",
    "        tof_data = batch[\"tof\"].to(device)\n",
    "        acc_data = batch[\"acc\"].to(device)\n",
    "        rot_data = batch[\"rot\"].to(device)\n",
    "        thm_data = batch[\"thm\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(tof_data, acc_data, rot_data, thm_data)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Statistics\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Update progress bar\n",
    "        train_acc = 100.0 * train_correct / train_total\n",
    "        train_pbar.set_postfix(\n",
    "            {\n",
    "                \"Loss\": f\"{loss.item():.4f}\",\n",
    "                \"Acc\": f\"{train_acc:.2f}%\",\n",
    "            },\n",
    "        )\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\")\n",
    "        for batch in val_pbar:\n",
    "            # Move to device\n",
    "            tof_data = batch[\"tof\"].to(device)\n",
    "            acc_data = batch[\"acc\"].to(device)\n",
    "            rot_data = batch[\"rot\"].to(device)\n",
    "            thm_data = batch[\"thm\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(tof_data, acc_data, rot_data, thm_data)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Statistics\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Update progress bar\n",
    "            val_acc = 100.0 * val_correct / val_total\n",
    "            val_pbar.set_postfix(\n",
    "                {\n",
    "                    \"Loss\": f\"{loss.item():.4f}\",\n",
    "                    \"Acc\": f\"{val_acc:.2f}%\",\n",
    "                },\n",
    "            )\n",
    "\n",
    "    # Calculate epoch metrics\n",
    "    epoch_train_loss = train_loss / len(train_loader)\n",
    "    epoch_train_acc = 100.0 * train_correct / train_total\n",
    "    epoch_val_loss = val_loss / len(val_loader)\n",
    "    epoch_val_acc = 100.0 * val_correct / val_total\n",
    "\n",
    "    # Store metrics\n",
    "    train_losses.append(epoch_train_loss)\n",
    "    train_accuracies.append(epoch_train_acc)\n",
    "    val_losses.append(epoch_val_loss)\n",
    "    val_accuracies.append(epoch_val_acc)\n",
    "\n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(epoch_val_acc)\n",
    "\n",
    "    # Save best model\n",
    "    if epoch_val_acc > best_val_acc:\n",
    "        best_val_acc = epoch_val_acc\n",
    "        torch.save(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"best_val_acc\": best_val_acc,\n",
    "                \"label_encoder\": label_encoder,\n",
    "            },\n",
    "            \"../models/best_model.pt\",\n",
    "        )\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "    print(f\"  Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.2f}%\")\n",
    "    print(f\"  Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.2f}%\")\n",
    "    print(f\"  Best Val Acc: {best_val_acc:.2f}%\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "print(f\"Training completed! Best validation accuracy: {best_val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss curves\n",
    "ax1.plot(train_losses, label=\"Train Loss\", color=\"blue\")\n",
    "ax1.plot(val_losses, label=\"Validation Loss\", color=\"red\")\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.set_title(\"Training and Validation Loss\")\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2.plot(train_accuracies, label=\"Train Accuracy\", color=\"blue\")\n",
    "ax2.plot(val_accuracies, label=\"Validation Accuracy\", color=\"red\")\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax2.set_ylabel(\"Accuracy (%)\")\n",
    "ax2.set_title(\"Training and Validation Accuracy\")\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Final Results:\")\n",
    "print(f\"  Final Train Accuracy: {train_accuracies[-1]:.2f}%\")\n",
    "print(f\"  Final Validation Accuracy: {val_accuracies[-1]:.2f}%\")\n",
    "print(f\"  Best Validation Accuracy: {best_val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model for evaluation\n",
    "checkpoint = torch.load(\"../models/best_model.pt\", map_location=device)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model.eval()\n",
    "\n",
    "# Evaluate on validation set\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "        # Move to device\n",
    "        tof_data = batch[\"tof\"].to(device)\n",
    "        acc_data = batch[\"acc\"].to(device)\n",
    "        rot_data = batch[\"rot\"].to(device)\n",
    "        thm_data = batch[\"thm\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(tof_data, acc_data, rot_data, thm_data)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "report = classification_report(\n",
    "    all_labels,\n",
    "    all_predictions,\n",
    "    target_names=label_encoder.classes_,\n",
    "    digits=3,\n",
    ")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "plt.figure(figsize=(16, 12))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=label_encoder.classes_,\n",
    "    yticklabels=label_encoder.classes_,\n",
    ")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze per-class performance\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "precision, recall, f1, support = precision_recall_fscore_support(\n",
    "    all_labels,\n",
    "    all_predictions,\n",
    "    average=None,\n",
    ")\n",
    "\n",
    "results_df = pl.DataFrame(\n",
    "    {\n",
    "        \"gesture\": label_encoder.classes_,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1,\n",
    "        \"support\": support,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Add target/non-target classification\n",
    "results_df = results_df.with_columns(\n",
    "    pl.when(pl.col(\"gesture\").is_in(target_gestures))\n",
    "    .then(pl.lit(\"Target\"))\n",
    "    .otherwise(pl.lit(\"Non-target\"))\n",
    "    .alias(\"category\"),\n",
    ")\n",
    "\n",
    "# Sort by F1 score\n",
    "results_df = results_df.sort(\"f1_score\", descending=True)\n",
    "print(\"Per-class Performance:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target vs Non-target performance\n",
    "target_avg = (\n",
    "    results_df.filter(pl.col(\"category\") == \"Target\")\n",
    "    .select([\"precision\", \"recall\", \"f1_score\"])\n",
    "    .mean()\n",
    ")\n",
    "non_target_avg = (\n",
    "    results_df.filter(pl.col(\"category\") == \"Non-target\")\n",
    "    .select([\"precision\", \"recall\", \"f1_score\"])\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "print(\"\\nAverage Performance by Category:\")\n",
    "print(\n",
    "    f\"Target gestures - Precision: {target_avg['precision'][0]:.3f}, Recall: {target_avg['recall'][0]:.3f}, F1: {target_avg['f1_score'][0]:.3f}\",\n",
    ")\n",
    "print(\n",
    "    f\"Non-target gestures - Precision: {non_target_avg['precision'][0]:.3f}, Recall: {non_target_avg['recall'][0]:.3f}, F1: {non_target_avg['f1_score'][0]:.3f}\",\n",
    ")\n",
    "\n",
    "# Overall metrics\n",
    "macro_avg = results_df.select([\"precision\", \"recall\", \"f1_score\"]).mean()\n",
    "print(\n",
    "    f\"\\nOverall Macro Average - Precision: {macro_avg['precision'][0]:.3f}, Recall: {macro_avg['recall'][0]:.3f}, F1: {macro_avg['f1_score'][0]:.3f}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Final Model and Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models directory if it doesn't exist\n",
    "os.makedirs(\"../models\", exist_ok=True)\n",
    "\n",
    "# Save final model with all metadata\n",
    "final_model_path = \"../models/cmi_gesture_model_final.pt\"\n",
    "torch.save(\n",
    "    {\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"model_config\": {\n",
    "            \"num_classes\": 18,\n",
    "            \"d_model\": 128,\n",
    "            \"num_heads\": 8,\n",
    "            \"max_seq_length\": max_seq_length,\n",
    "        },\n",
    "        \"label_encoder\": label_encoder,\n",
    "        \"training_history\": {\n",
    "            \"train_losses\": train_losses,\n",
    "            \"train_accuracies\": train_accuracies,\n",
    "            \"val_losses\": val_losses,\n",
    "            \"val_accuracies\": val_accuracies,\n",
    "        },\n",
    "        \"final_metrics\": {\n",
    "            \"best_val_accuracy\": best_val_acc,\n",
    "            \"final_val_accuracy\": accuracy,\n",
    "            \"macro_precision\": macro_avg[\"precision\"][0],\n",
    "            \"macro_recall\": macro_avg[\"recall\"][0],\n",
    "            \"macro_f1\": macro_avg[\"f1_score\"][0],\n",
    "        },\n",
    "        \"feature_columns\": {\n",
    "            \"tof_cols\": tof_cols,\n",
    "            \"acc_cols\": acc_cols,\n",
    "            \"rot_cols\": rot_cols,\n",
    "            \"thm_cols\": thm_cols,\n",
    "        },\n",
    "    },\n",
    "    final_model_path,\n",
    ")\n",
    "\n",
    "print(f\"Final model saved to: {final_model_path}\")\n",
    "print(f\"Model file size: {os.path.getsize(final_model_path) / (1024*1024):.1f} MB\")\n",
    "\n",
    "print(\"\\n=== Training Summary ===\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Max sequence length: {max_seq_length}\")\n",
    "print(f\"Training sequences: {len(train_sequences)}\")\n",
    "print(f\"Validation sequences: {len(val_sequences)}\")\n",
    "print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "print(f\"Final validation accuracy: {accuracy*100:.2f}%\")\n",
    "print(\"Training completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
