{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMI Gesture Recognition - Inference Notebook\n",
    "\n",
    "This notebook demonstrates inference with the trained model, handling sequence chunking and aggregation for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path().absolute().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "from collections import defaultdict\n",
    "\n",
    "from src.dataset import CMIDataset, SequenceProcessor, prepare_gesture_labels\n",
    "from src.model import create_model\n",
    "from src.trainer import CMITrainer\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Configuration and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config\n",
    "with open(project_root / 'config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"- Model d_model: {config['model']['d_model']}\")\n",
    "print(f\"- Model layers: {config['model']['num_layers']}\")\n",
    "print(f\"- Max sequence length for chunking: {config['data']['max_seq_length']}\")\n",
    "print(f\"- Max sequence length for positional encoding: {config['model']['max_seq_length']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best trained model\n",
    "experiment_dirs = list((project_root / 'experiments').glob('cmi_training_*'))\n",
    "if not experiment_dirs:\n",
    "    raise FileNotFoundError(\"No training experiments found\")\n",
    "\n",
    "# Use the most recent experiment\n",
    "latest_experiment = max(experiment_dirs, key=lambda x: x.name)\n",
    "model_path = latest_experiment / 'models' / 'best_model.pt'\n",
    "\n",
    "print(f\"Loading model from: {model_path}\")\n",
    "\n",
    "# Load the model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "\n",
    "# Create model with saved configuration\n",
    "model_config = checkpoint['model_config']\n",
    "model = create_model(**model_config).to(device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded successfully on {device}\")\n",
    "print(f\"Model config: {model_config}\")\n",
    "\n",
    "# Load label encoder if available\n",
    "label_encoder = checkpoint.get('label_encoder')\n",
    "if label_encoder is None:\n",
    "    print(\"Warning: No label encoder found in checkpoint\")\n",
    "else:\n",
    "    print(f\"Label encoder loaded with {len(label_encoder.classes_)} classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "test_df = pl.read_csv(project_root / 'dataset' / 'test.csv')\n",
    "test_demographics = pl.read_csv(project_root / 'dataset' / 'test_demographics.csv')\n",
    "\n",
    "# Merge with demographics if needed\n",
    "if 'participant_id' in test_df.columns and 'participant_id' in test_demographics.columns:\n",
    "    test_df = test_df.join(test_demographics, on='participant_id', how='left')\n",
    "\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "print(f\"Unique sequences: {test_df['sequence_id'].n_unique()}\")\n",
    "print(f\"Columns: {test_df.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Inference Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceProcessor:\n",
    "    \"\"\"Process sequences for inference with proper chunking and aggregation.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, device, max_chunk_length=20):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.max_chunk_length = max_chunk_length\n",
    "        self.sequence_processor = SequenceProcessor()\n",
    "    \n",
    "    def process_sequence_for_inference(self, sequence_data):\n",
    "        \"\"\"Process a single sequence, creating chunks if necessary.\"\"\"\n",
    "        sequence_id = sequence_data['sequence_id'][0]\n",
    "        \n",
    "        try:\n",
    "            # Create enhanced features using FeatureProcessor\n",
    "            enhanced_features = self.sequence_processor.feature_processor.create_sequence_features(\n",
    "                sequence_data\n",
    "            )\n",
    "            \n",
    "            # Apply chunking for inference\n",
    "            chunks = self._chunk_sequence_for_inference(\n",
    "                enhanced_features, sequence_id, self.max_chunk_length\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing enhanced features for {sequence_id}, falling back to original: {e}\")\n",
    "            # Fallback to original processing\n",
    "            acc_cols = [\"acc_x\", \"acc_y\", \"acc_z\"]\n",
    "            rot_cols = [\"rot_w\", \"rot_x\", \"rot_y\", \"rot_z\"]\n",
    "            thm_cols = [f\"thm_{i}\" for i in range(1, 6)]\n",
    "            tof_cols = [f\"tof_{i}_v{j}\" for i in range(1, 6) for j in range(64)]\n",
    "            \n",
    "            seq_data = sequence_data.select(\n",
    "                acc_cols + rot_cols + thm_cols + tof_cols\n",
    "            ).to_numpy()\n",
    "            \n",
    "            chunks = self._chunk_original_sequence_for_inference(\n",
    "                seq_data, sequence_id, self.max_chunk_length\n",
    "            )\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _chunk_sequence_for_inference(self, enhanced_features, sequence_id, max_seq_length):\n",
    "        \"\"\"Chunk enhanced features for inference.\"\"\"\n",
    "        tof_data = enhanced_features[\"tof\"]\n",
    "        acc_data = enhanced_features[\"acc\"]\n",
    "        rot_data = enhanced_features[\"rot\"]\n",
    "        thm_data = enhanced_features[\"thm\"]\n",
    "        \n",
    "        seq_length = len(tof_data)\n",
    "        chunks = []\n",
    "        \n",
    "        if seq_length <= max_seq_length:\n",
    "            # No chunking needed\n",
    "            chunks.append({\n",
    "                \"sequence_id\": sequence_id,\n",
    "                \"original_sequence_id\": sequence_id,\n",
    "                \"enhanced_data\": enhanced_features,\n",
    "                \"chunk_start_idx\": 0,\n",
    "            })\n",
    "        else:\n",
    "            # Split into chunks\n",
    "            num_chunks = (seq_length + max_seq_length - 1) // max_seq_length\n",
    "            for i in range(num_chunks):\n",
    "                start_idx = i * max_seq_length\n",
    "                end_idx = min((i + 1) * max_seq_length, seq_length)\n",
    "                \n",
    "                chunk_features = {\n",
    "                    \"tof\": tof_data[start_idx:end_idx],\n",
    "                    \"acc\": acc_data[start_idx:end_idx],\n",
    "                    \"rot\": rot_data[start_idx:end_idx],\n",
    "                    \"thm\": thm_data[start_idx:end_idx],\n",
    "                }\n",
    "                \n",
    "                chunks.append({\n",
    "                    \"sequence_id\": f\"{sequence_id}_chunk_{i}\",\n",
    "                    \"original_sequence_id\": sequence_id,\n",
    "                    \"enhanced_data\": chunk_features,\n",
    "                    \"chunk_start_idx\": start_idx,\n",
    "                })\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _chunk_original_sequence_for_inference(self, seq_data, sequence_id, max_seq_length):\n",
    "        \"\"\"Chunk original sequence data for inference.\"\"\"\n",
    "        seq_length = len(seq_data)\n",
    "        chunks = []\n",
    "        \n",
    "        if seq_length <= max_seq_length:\n",
    "            # No chunking needed\n",
    "            chunks.append({\n",
    "                \"sequence_id\": sequence_id,\n",
    "                \"original_sequence_id\": sequence_id,\n",
    "                \"data\": seq_data,\n",
    "                \"chunk_start_idx\": 0,\n",
    "            })\n",
    "        else:\n",
    "            # Split into chunks\n",
    "            num_chunks = (seq_length + max_seq_length - 1) // max_seq_length\n",
    "            for i in range(num_chunks):\n",
    "                start_idx = i * max_seq_length\n",
    "                end_idx = min((i + 1) * max_seq_length, seq_length)\n",
    "                \n",
    "                chunks.append({\n",
    "                    \"sequence_id\": f\"{sequence_id}_chunk_{i}\",\n",
    "                    \"original_sequence_id\": sequence_id,\n",
    "                    \"data\": seq_data[start_idx:end_idx],\n",
    "                    \"chunk_start_idx\": start_idx,\n",
    "                })\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def predict_sequences(self, test_df, batch_size=16):\n",
    "        \"\"\"Predict on test sequences with proper aggregation.\"\"\"\n",
    "        # Group by sequence_id\n",
    "        grouped = test_df.group_by(\"sequence_id\")\n",
    "        \n",
    "        # Process all sequences into chunks\n",
    "        all_chunks = []\n",
    "        sequence_to_chunks = defaultdict(list)\n",
    "        \n",
    "        print(\"Processing sequences into chunks...\")\n",
    "        for seq_id, group in tqdm(grouped):\n",
    "            chunks = self.process_sequence_for_inference(group)\n",
    "            for chunk in chunks:\n",
    "                all_chunks.append(chunk)\n",
    "                sequence_to_chunks[chunk[\"original_sequence_id\"]].append(len(all_chunks) - 1)\n",
    "        \n",
    "        print(f\"Created {len(all_chunks)} chunks from {len(sequence_to_chunks)} sequences\")\n",
    "        \n",
    "        # Create dataset and dataloader\n",
    "        dataset = CMIDataset(\n",
    "            all_chunks,\n",
    "            max_length=self.max_chunk_length\n",
    "        )\n",
    "        \n",
    "        dataloader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0\n",
    "        )\n",
    "        \n",
    "        # Run inference on all chunks\n",
    "        chunk_predictions = []\n",
    "        chunk_probabilities = []\n",
    "        \n",
    "        print(\"Running inference on chunks...\")\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dataloader):\n",
    "                # Move to device\n",
    "                tof_data = batch[\"tof\"].to(self.device)\n",
    "                acc_data = batch[\"acc\"].to(self.device)\n",
    "                rot_data = batch[\"rot\"].to(self.device)\n",
    "                thm_data = batch[\"thm\"].to(self.device)\n",
    "                chunk_start_idx = batch.get(\"chunk_start_idx\")\n",
    "                if chunk_start_idx is not None:\n",
    "                    chunk_start_idx = chunk_start_idx.to(self.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = self.model(\n",
    "                    tof_data, acc_data, rot_data, thm_data, chunk_start_idx\n",
    "                )\n",
    "                \n",
    "                # Get probabilities and predictions\n",
    "                probabilities = F.softmax(outputs, dim=1)\n",
    "                predictions = torch.argmax(outputs, dim=1)\n",
    "                \n",
    "                chunk_predictions.extend(predictions.cpu().numpy())\n",
    "                chunk_probabilities.extend(probabilities.cpu().numpy())\n",
    "        \n",
    "        # Aggregate predictions by original sequence_id\n",
    "        print(\"Aggregating predictions by sequence...\")\n",
    "        sequence_predictions = {}\n",
    "        \n",
    "        for original_seq_id, chunk_indices in sequence_to_chunks.items():\n",
    "            # Get probabilities for all chunks of this sequence\n",
    "            chunk_probs = [chunk_probabilities[i] for i in chunk_indices]\n",
    "            \n",
    "            # Average probabilities across chunks\n",
    "            avg_probs = np.mean(chunk_probs, axis=0)\n",
    "            \n",
    "            # Get final prediction\n",
    "            final_prediction = np.argmax(avg_probs)\n",
    "            \n",
    "            sequence_predictions[original_seq_id] = {\n",
    "                'prediction': final_prediction,\n",
    "                'probabilities': avg_probs,\n",
    "                'num_chunks': len(chunk_indices)\n",
    "            }\n",
    "        \n",
    "        return sequence_predictions\n",
    "\n",
    "# Initialize inference processor\n",
    "inference_processor = InferenceProcessor(\n",
    "    model=model, \n",
    "    device=device, \n",
    "    max_chunk_length=config['data']['max_seq_length']\n",
    ")\n",
    "\n",
    "print(\"Inference processor initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Inference on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference\n",
    "predictions = inference_processor.predict_sequences(test_df, batch_size=16)\n",
    "\n",
    "print(f\"Generated predictions for {len(predictions)} sequences\")\n",
    "\n",
    "# Show some example predictions\n",
    "for i, (seq_id, pred_info) in enumerate(list(predictions.items())[:5]):\n",
    "    print(f\"Sequence {seq_id}:\")\n",
    "    print(f\"  Prediction: {pred_info['prediction']}\")\n",
    "    print(f\"  Max probability: {pred_info['probabilities'].max():.4f}\")\n",
    "    print(f\"  Number of chunks: {pred_info['num_chunks']}\")\n",
    "    if label_encoder is not None:\n",
    "        gesture_name = label_encoder.inverse_transform([pred_info['prediction']])[0]\n",
    "        print(f\"  Gesture: {gesture_name}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze prediction distribution\n",
    "prediction_counts = defaultdict(int)\n",
    "chunk_counts = defaultdict(int)\n",
    "confidence_scores = []\n",
    "\n",
    "for seq_id, pred_info in predictions.items():\n",
    "    prediction_counts[pred_info['prediction']] += 1\n",
    "    chunk_counts[pred_info['num_chunks']] += 1\n",
    "    confidence_scores.append(pred_info['probabilities'].max())\n",
    "\n",
    "print(\"Prediction distribution:\")\n",
    "for pred, count in sorted(prediction_counts.items()):\n",
    "    if label_encoder is not None:\n",
    "        gesture_name = label_encoder.inverse_transform([pred])[0]\n",
    "        print(f\"  Class {pred} ({gesture_name}): {count} sequences\")\n",
    "    else:\n",
    "        print(f\"  Class {pred}: {count} sequences\")\n",
    "\n",
    "print(\"\\nChunk distribution:\")\n",
    "for num_chunks, count in sorted(chunk_counts.items()):\n",
    "    print(f\"  {num_chunks} chunks: {count} sequences\")\n",
    "\n",
    "print(f\"\\nConfidence statistics:\")\n",
    "print(f\"  Mean confidence: {np.mean(confidence_scores):.4f}\")\n",
    "print(f\"  Median confidence: {np.median(confidence_scores):.4f}\")\n",
    "print(f\"  Min confidence: {np.min(confidence_scores):.4f}\")\n",
    "print(f\"  Max confidence: {np.max(confidence_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission dataframe\n",
    "submission_data = []\n",
    "\n",
    "for seq_id, pred_info in predictions.items():\n",
    "    submission_data.append({\n",
    "        'sequence_id': seq_id,\n",
    "        'prediction': pred_info['prediction'],\n",
    "        'confidence': pred_info['probabilities'].max(),\n",
    "        'num_chunks': pred_info['num_chunks']\n",
    "    })\n",
    "    \n",
    "    # Add probability columns if needed\n",
    "    for class_idx, prob in enumerate(pred_info['probabilities']):\n",
    "        submission_data[-1][f'prob_class_{class_idx}'] = prob\n",
    "\n",
    "# Convert to DataFrame\n",
    "submission_df = pd.DataFrame(submission_data)\n",
    "\n",
    "# Save predictions\n",
    "output_dir = project_root / 'predictions'\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "submission_path = output_dir / 'test_predictions.csv'\n",
    "submission_df.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"Predictions saved to: {submission_path}\")\n",
    "print(f\"Submission shape: {submission_df.shape}\")\n",
    "\n",
    "# Display first few predictions\n",
    "display_cols = ['sequence_id', 'prediction', 'confidence', 'num_chunks']\n",
    "if label_encoder is not None:\n",
    "    submission_df['gesture_name'] = label_encoder.inverse_transform(submission_df['prediction'])\n",
    "    display_cols.append('gesture_name')\n",
    "\n",
    "print(\"\\nFirst 10 predictions:\")\n",
    "print(submission_df[display_cols].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates:\n",
    "\n",
    "1. **Proper sequence chunking**: Test sequences are split into chunks of the same length used during training\n",
    "2. **Chunk-aware inference**: Each chunk is processed with appropriate positional encoding based on its position in the original sequence\n",
    "3. **Prediction aggregation**: Multiple chunk predictions for the same sequence are aggregated by averaging probabilities and taking the argmax\n",
    "4. **Confidence estimation**: The maximum probability across classes provides a confidence measure\n",
    "\n",
    "Key differences from training:\n",
    "- During training: sequences are chunked and each chunk gets a separate label\n",
    "- During inference: sequences are chunked but predictions are aggregated back to the original sequence level\n",
    "\n",
    "This approach ensures that long sequences are handled consistently between training and inference while properly accounting for sequence structure."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
