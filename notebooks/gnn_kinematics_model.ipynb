{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNN Kinematics Model for CMI Gesture Classification\n",
    "\n",
    "This notebook implements a novel Graph Neural Network approach for gesture classification based on kinematic modeling:\n",
    "\n",
    "1. **Virtual Kinematic Chain**: Model shoulder→elbow→wrist joint relationships\n",
    "2. **Gesture Generation**: Generate expected angular velocity patterns for each gesture\n",
    "3. **Comparison-based Classification**: Compare generated patterns with actual sensor data\n",
    "4. **Demographics Integration**: Adapt kinematic parameters based on body measurements\n",
    "\n",
    "**Key Innovation**: Instead of direct classification, we generate expected motion patterns for each gesture and find the best match, providing physically interpretable predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports\n",
    "import os, json, joblib, numpy as np, pandas as pd\n",
    "import random\n",
    "from pathlib import Path\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Deep learning and GNN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import Adam, AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Graph neural networks\n",
    "import torch_geometric\n",
    "from torch_geometric.nn import GCNConv, GATConv, TransformerConv\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "# Scientific computing\n",
    "import polars as pl\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from scipy.spatial.distance import euclidean\n",
    "from scipy.stats import pearsonr\n",
    "from fastdtw import fastdtw\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def seed_everything(seed=42):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything(42)\n",
    "print(\"✅ Setup complete - GNN Kinematics Model\")\n",
    "print(f\"   PyTorch version: {torch.__version__}\")\n",
    "print(f\"   PyTorch Geometric version: {torch_geometric.__version__}\")\n",
    "print(f\"   Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    'TRAIN_MODE': True,\n",
    "    'USE_LOCAL_DATA': True,\n",
    "    'DEVICE': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'RANDOM_SEED': 42,\n",
    "    'BATCH_SIZE': 32,\n",
    "    'LEARNING_RATE': 1e-3,\n",
    "    'N_EPOCHS': 100,\n",
    "    'PATIENCE': 20,\n",
    "    'SAMPLE_RATE': 200,  # Hz\n",
    "    'MAX_SEQUENCE_LENGTH': 200,\n",
    "    'MIN_SEQUENCE_LENGTH': 10\n",
    "}\n",
    "\n",
    "# Data paths\n",
    "if CONFIG['USE_LOCAL_DATA']:\n",
    "    DATA_DIR = Path(\"../dataset\")\n",
    "    MODELS_DIR = Path(\"../models\")\n",
    "else:\n",
    "    DATA_DIR = Path(\"/kaggle/input/cmi-detect-behavior-with-sensor-data\")\n",
    "    MODELS_DIR = Path(\"/kaggle/input/pretrained-models\")\n",
    "\n",
    "OUTPUT_DIR = Path(\"../results/gnn_kinematics\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Data file paths\n",
    "DATA_PATHS = {\n",
    "    'train_data': DATA_DIR / \"train.csv\",\n",
    "    'train_demographics': DATA_DIR / \"train_demographics.csv\",\n",
    "    'test_data': DATA_DIR / \"test.csv\",\n",
    "    'test_demographics': DATA_DIR / \"test_demographics.csv\"\n",
    "}\n",
    "\n",
    "# Gesture classes\n",
    "GESTURE_CLASSES = [\n",
    "    'Above ear - pull hair', 'Cheek - pinch skin', 'Drink from bottle/cup',\n",
    "    'Eyebrow - pull hair', 'Eyelash - pull hair', 'Feel around in tray and pull out an object',\n",
    "    'Forehead - pull hairline', 'Forehead - scratch', 'Glasses on/off',\n",
    "    'Neck - pinch skin', 'Neck - scratch', 'Pinch knee/leg skin',\n",
    "    'Pull air toward your face', 'Scratch knee/leg skin', 'Text on phone',\n",
    "    'Wave hello', 'Write name in air', 'Write name on leg'\n",
    "]\n",
    "\n",
    "# Demographics features\n",
    "DEMOGRAPHICS_FEATURES = [\n",
    "    'adult_child', 'age', 'sex', 'handedness', 'height_cm', \n",
    "    'shoulder_to_wrist_cm', 'elbow_to_wrist_cm'\n",
    "]\n",
    "\n",
    "# Kinematic model parameters\n",
    "KINEMATIC_CONFIG = {\n",
    "    'n_joints': 3,  # shoulder, elbow, wrist\n",
    "    'hidden_dim': 128,\n",
    "    'gnn_layers': 3,\n",
    "    'attention_heads': 4,\n",
    "    'dropout': 0.1,\n",
    "    'joint_dof': [3, 1, 2],  # Degrees of freedom for each joint\n",
    "    'physics_weight': 0.1,  # Weight for physics-based constraints\n",
    "}\n",
    "\n",
    "print(f\"✅ Configuration loaded\")\n",
    "print(f\"   Device: {CONFIG['DEVICE']}\")\n",
    "print(f\"   Data directory: {DATA_DIR}\")\n",
    "print(f\"   Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"   Number of gesture classes: {len(GESTURE_CLASSES)}\")\n",
    "print(f\"   Kinematic joints: {KINEMATIC_CONFIG['n_joints']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering and Data Processing\n",
    "\n",
    "Essential functions for processing sensor data and extracting angular velocities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_gravity_from_acc(acc_data, rot_data):\n",
    "    \"\"\"Remove gravity component from accelerometer data using quaternion rotation\"\"\"\n",
    "    if isinstance(acc_data, pd.DataFrame):\n",
    "        acc_values = acc_data[['acc_x', 'acc_y', 'acc_z']].values\n",
    "    else:\n",
    "        acc_values = acc_data\n",
    "\n",
    "    if isinstance(rot_data, pd.DataFrame):\n",
    "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    else:\n",
    "        quat_values = rot_data\n",
    "\n",
    "    num_samples = acc_values.shape[0]\n",
    "    linear_accel = np.zeros_like(acc_values)\n",
    "    gravity_world = np.array([0, 0, 9.81])\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        if np.all(np.isnan(quat_values[i])) or np.all(np.isclose(quat_values[i], 0)):\n",
    "            linear_accel[i, :] = acc_values[i, :] \n",
    "            continue\n",
    "        try:\n",
    "            rotation = R.from_quat(quat_values[i])\n",
    "            gravity_sensor_frame = rotation.apply(gravity_world, inverse=True)\n",
    "            linear_accel[i, :] = acc_values[i, :] - gravity_sensor_frame\n",
    "        except ValueError:\n",
    "             linear_accel[i, :] = acc_values[i, :]\n",
    "             \n",
    "    return linear_accel\n",
    "\n",
    "def calculate_angular_velocity_from_quat(rot_data, time_delta=None):\n",
    "    \"\"\"Calculate angular velocity from quaternion data\"\"\"\n",
    "    if time_delta is None:\n",
    "        time_delta = 1.0 / CONFIG['SAMPLE_RATE']\n",
    "    \n",
    "    if isinstance(rot_data, pd.DataFrame):\n",
    "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    else:\n",
    "        quat_values = rot_data\n",
    "\n",
    "    num_samples = quat_values.shape[0]\n",
    "    angular_vel = np.zeros((num_samples, 3))\n",
    "\n",
    "    for i in range(num_samples - 1):\n",
    "        q_t = quat_values[i]\n",
    "        q_t_plus_dt = quat_values[i+1]\n",
    "\n",
    "        if np.all(np.isnan(q_t)) or np.all(np.isclose(q_t, 0)) or \\\n",
    "           np.all(np.isnan(q_t_plus_dt)) or np.all(np.isclose(q_t_plus_dt, 0)):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            rot_t = R.from_quat(q_t)\n",
    "            rot_t_plus_dt = R.from_quat(q_t_plus_dt)\n",
    "            delta_rot = rot_t.inv() * rot_t_plus_dt\n",
    "            angular_vel[i, :] = delta_rot.as_rotvec() / time_delta\n",
    "        except ValueError:\n",
    "            pass\n",
    "            \n",
    "    return angular_vel\n",
    "\n",
    "def calculate_angular_distance(rot_data):\n",
    "    \"\"\"Calculate angular distance between consecutive quaternions\"\"\"\n",
    "    if isinstance(rot_data, pd.DataFrame):\n",
    "        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n",
    "    else:\n",
    "        quat_values = rot_data\n",
    "\n",
    "    num_samples = quat_values.shape[0]\n",
    "    angular_dist = np.zeros(num_samples)\n",
    "\n",
    "    for i in range(num_samples - 1):\n",
    "        q1 = quat_values[i]\n",
    "        q2 = quat_values[i+1]\n",
    "\n",
    "        if np.all(np.isnan(q1)) or np.all(np.isclose(q1, 0)) or \\\n",
    "           np.all(np.isnan(q2)) or np.all(np.isclose(q2, 0)):\n",
    "            angular_dist[i] = 0\n",
    "            continue\n",
    "        try:\n",
    "            r1 = R.from_quat(q1)\n",
    "            r2 = R.from_quat(q2)\n",
    "            relative_rotation = r1.inv() * r2\n",
    "            angle = np.linalg.norm(relative_rotation.as_rotvec())\n",
    "            angular_dist[i] = angle\n",
    "        except ValueError:\n",
    "            angular_dist[i] = 0\n",
    "            pass\n",
    "            \n",
    "    return angular_dist\n",
    "\n",
    "def extract_sequence_features(sequence_data):\n",
    "    \"\"\"Extract angular velocity and other kinematic features from sequence\"\"\"\n",
    "    try:\n",
    "        if hasattr(sequence_data, 'to_pandas'):\n",
    "            df = sequence_data.to_pandas()\n",
    "        else:\n",
    "            df = sequence_data.copy()\n",
    "        \n",
    "        # Calculate angular velocity\n",
    "        angular_velocity = calculate_angular_velocity_from_quat(df)\n",
    "        \n",
    "        # Calculate linear acceleration (gravity removed)\n",
    "        linear_accel = remove_gravity_from_acc(df, df)\n",
    "        \n",
    "        # Calculate angular distance\n",
    "        angular_distance = calculate_angular_distance(df)\n",
    "        \n",
    "        # Combine features\n",
    "        features = {\n",
    "            'angular_velocity': angular_velocity,\n",
    "            'linear_acceleration': linear_accel,\n",
    "            'angular_distance': angular_distance,\n",
    "            'timestamp': np.arange(len(df)) / CONFIG['SAMPLE_RATE']\n",
    "        }\n",
    "        \n",
    "        return features\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Feature extraction failed: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"✅ Feature engineering functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Virtual Kinematic Chain Model\n",
    "\n",
    "The core GNN model that simulates the shoulder→elbow→wrist kinematic chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VirtualKinematicChain(nn.Module):\n",
    "    \"\"\"Virtual kinematic chain model using GNN for gesture simulation\"\"\"\n",
    "    \n",
    "    def __init__(self, n_joints=3, hidden_dim=128, n_classes=18, \n",
    "                 gnn_layers=3, attention_heads=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_joints = n_joints\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_classes = n_classes\n",
    "        self.gnn_layers = gnn_layers\n",
    "        \n",
    "        # Gesture embeddings\n",
    "        self.gesture_embeddings = nn.Embedding(n_classes, hidden_dim)\n",
    "        \n",
    "        # Demographics encoder\n",
    "        self.demo_encoder = nn.Sequential(\n",
    "            nn.Linear(len(DEMOGRAPHICS_FEATURES), 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Joint-specific initializers\n",
    "        self.joint_initializers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ) for _ in range(n_joints)\n",
    "        ])\n",
    "        \n",
    "        # Graph neural network layers\n",
    "        self.gnn_convs = nn.ModuleList()\n",
    "        self.gnn_norms = nn.ModuleList()\n",
    "        \n",
    "        for i in range(gnn_layers):\n",
    "            # Use Graph Attention Network for better expressiveness\n",
    "            self.gnn_convs.append(\n",
    "                GATConv(hidden_dim, hidden_dim // attention_heads, \n",
    "                       heads=attention_heads, dropout=dropout, concat=True)\n",
    "            )\n",
    "            self.gnn_norms.append(nn.LayerNorm(hidden_dim))\n",
    "        \n",
    "        # Temporal modeling\n",
    "        self.temporal_lstm = nn.LSTM(\n",
    "            hidden_dim, hidden_dim, batch_first=True, dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # Physics-informed constraints\n",
    "        self.physics_constraint = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, 3)  # 3D angular velocity constraints\n",
    "        )\n",
    "        \n",
    "        # Angular velocity prediction head (for wrist)\n",
    "        self.angular_velocity_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 3)  # 3D angular velocity\n",
    "        )\n",
    "        \n",
    "        # Build kinematic graph (shoulder -> elbow -> wrist)\n",
    "        self.register_buffer('edge_index', \n",
    "                           torch.tensor([[0, 1], [1, 2]], dtype=torch.long).t().contiguous())\n",
    "        \n",
    "        print(f\"✅ VirtualKinematicChain initialized\")\n",
    "        print(f\"   Joints: {n_joints}, Hidden dim: {hidden_dim}\")\n",
    "        print(f\"   GNN layers: {gnn_layers}, Attention heads: {attention_heads}\")\n",
    "        \n",
    "    def forward(self, gesture_idx, demographics, sequence_length, return_intermediates=False):\n",
    "        \"\"\"Generate angular velocity sequence for given gesture and demographics\"\"\"\n",
    "        batch_size = gesture_idx.shape[0]\n",
    "        device = gesture_idx.device\n",
    "        \n",
    "        # Encode gesture and demographics\n",
    "        gesture_emb = self.gesture_embeddings(gesture_idx)  # (batch, hidden_dim)\n",
    "        demo_emb = self.demo_encoder(demographics)  # (batch, hidden_dim)\n",
    "        \n",
    "        # Initialize joint features\n",
    "        joint_features = []\n",
    "        combined_emb = torch.cat([gesture_emb, demo_emb], dim=1)  # (batch, hidden_dim*2)\n",
    "        \n",
    "        for i in range(self.n_joints):\n",
    "            joint_feat = self.joint_initializers[i](combined_emb)\n",
    "            joint_features.append(joint_feat)\n",
    "        \n",
    "        joint_features = torch.stack(joint_features, dim=1)  # (batch, n_joints, hidden_dim)\n",
    "        \n",
    "        # Generate sequence of angular velocities\n",
    "        angular_velocities = []\n",
    "        intermediate_states = [] if return_intermediates else None\n",
    "        \n",
    "        # LSTM hidden state for temporal consistency\n",
    "        lstm_hidden = None\n",
    "        \n",
    "        for t in range(sequence_length):\n",
    "            # Flatten for GNN processing\n",
    "            x = joint_features.view(-1, self.hidden_dim)  # (batch*n_joints, hidden_dim)\n",
    "            \n",
    "            # Create batch-aware edge index\n",
    "            edge_index = self._create_batch_edge_index(batch_size, device)\n",
    "            \n",
    "            # Apply GNN layers\n",
    "            for conv, norm in zip(self.gnn_convs, self.gnn_norms):\n",
    "                x_new = conv(x, edge_index)\n",
    "                x_new = norm(x_new)\n",
    "                x = F.relu(x_new) + x  # Residual connection\n",
    "            \n",
    "            # Reshape back to (batch, n_joints, hidden_dim)\n",
    "            x = x.view(batch_size, self.n_joints, self.hidden_dim)\n",
    "            \n",
    "            # Temporal modeling with LSTM\n",
    "            wrist_features = x[:, -1:, :]  # Extract wrist joint (last joint)\n",
    "            lstm_out, lstm_hidden = self.temporal_lstm(wrist_features, lstm_hidden)\n",
    "            wrist_features = lstm_out.squeeze(1)  # (batch, hidden_dim)\n",
    "            \n",
    "            # Apply physics constraints\n",
    "            physics_constraint = self.physics_constraint(wrist_features)\n",
    "            \n",
    "            # Predict angular velocity for this timestep\n",
    "            angular_vel = self.angular_velocity_head(wrist_features)  # (batch, 3)\n",
    "            \n",
    "            # Apply physics constraints (soft constraints)\n",
    "            angular_vel = angular_vel + KINEMATIC_CONFIG['physics_weight'] * physics_constraint\n",
    "            \n",
    "            angular_velocities.append(angular_vel)\n",
    "            \n",
    "            if return_intermediates:\n",
    "                intermediate_states.append({\n",
    "                    'joint_features': x.clone(),\n",
    "                    'wrist_features': wrist_features.clone(),\n",
    "                    'physics_constraint': physics_constraint.clone()\n",
    "                })\n",
    "            \n",
    "            # Update joint features with some dynamics (learned evolution)\n",
    "            # Add small learned perturbation for next timestep\n",
    "            evolution_noise = 0.05 * torch.randn_like(x) * (t / sequence_length)\n",
    "            joint_features = x + evolution_noise\n",
    "        \n",
    "        # Stack to create sequence\n",
    "        angular_velocities = torch.stack(angular_velocities, dim=1)  # (batch, seq_len, 3)\n",
    "        \n",
    "        if return_intermediates:\n",
    "            return angular_velocities, intermediate_states\n",
    "        else:\n",
    "            return angular_velocities\n",
    "    \n",
    "    def _create_batch_edge_index(self, batch_size, device):\n",
    "        \"\"\"Create edge index for batched graphs\"\"\"\n",
    "        batch_edge_indices = []\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            offset = b * self.n_joints\n",
    "            batch_edges = self.edge_index + offset\n",
    "            batch_edge_indices.append(batch_edges)\n",
    "        \n",
    "        return torch.cat(batch_edge_indices, dim=1).to(device)\n",
    "    \n",
    "    def visualize_kinematic_graph(self):\n",
    "        \"\"\"Visualize the kinematic graph structure\"\"\"\n",
    "        # Convert to networkx for visualization\n",
    "        edge_list = self.edge_index.t().cpu().numpy()\n",
    "        G = nx.DiGraph()\n",
    "        G.add_edges_from(edge_list)\n",
    "        \n",
    "        # Create layout\n",
    "        pos = {0: (0, 0), 1: (1, 0), 2: (2, 0)}  # Linear layout for shoulder->elbow->wrist\n",
    "        \n",
    "        plt.figure(figsize=(8, 4))\n",
    "        nx.draw(G, pos, with_labels=True, node_color='lightblue', \n",
    "                node_size=1500, arrowsize=20, font_size=12)\n",
    "        \n",
    "        # Add labels\n",
    "        labels = {0: 'Shoulder', 1: 'Elbow', 2: 'Wrist'}\n",
    "        nx.draw_networkx_labels(G, pos, labels, font_size=10)\n",
    "        \n",
    "        plt.title('Virtual Kinematic Chain: Shoulder → Elbow → Wrist')\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "print(\"✅ Virtual kinematic chain model defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GNN Gesture Classifier\n",
    "\n",
    "The main classifier that uses the kinematic model for gesture recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNGestureClassifier:\n",
    "    \"\"\"GNN-based gesture classifier using kinematic simulation and comparison\"\"\"\n",
    "    \n",
    "    def __init__(self, kinematic_model, gesture_classes=None, device='cpu'):\n",
    "        self.kinematic_model = kinematic_model.to(device)\n",
    "        self.gesture_classes = gesture_classes or GESTURE_CLASSES\n",
    "        self.n_classes = len(self.gesture_classes)\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.gesture_classes)}\n",
    "        self.device = device\n",
    "        \n",
    "        # Scaling for features\n",
    "        self.angular_velocity_scaler = StandardScaler()\n",
    "        self.demographics_scaler = StandardScaler()\n",
    "        \n",
    "        # Training history\n",
    "        self.training_history = {\n",
    "            'train_loss': [],\n",
    "            'val_loss': [],\n",
    "            'learning_rate': []\n",
    "        }\n",
    "        \n",
    "        print(f\"✅ GNN Gesture Classifier initialized\")\n",
    "        print(f\"   Device: {device}\")\n",
    "        print(f\"   Gesture classes: {self.n_classes}\")\n",
    "    \n",
    "    def prepare_training_data(self, train_sequences, train_demographics, \n",
    "                            sample_limit=None, verbose=True):\n",
    "        \"\"\"Prepare training data for GNN model\"\"\"\n",
    "        if verbose:\n",
    "            print(\"Preparing GNN training data...\")\n",
    "        \n",
    "        training_samples = []\n",
    "        failed_sequences = 0\n",
    "        \n",
    "        # Group sequences\n",
    "        if hasattr(train_sequences, 'group_by'):\n",
    "            sequence_groups = list(train_sequences.group_by('sequence_id'))\n",
    "        else:\n",
    "            sequence_groups = list(train_sequences.groupby('sequence_id'))\n",
    "        \n",
    "        # Limit samples if specified\n",
    "        if sample_limit and len(sequence_groups) > sample_limit:\n",
    "            sequence_groups = sequence_groups[:sample_limit]\n",
    "            if verbose:\n",
    "                print(f\"   Limited to {sample_limit} sequences\")\n",
    "        \n",
    "        # Process sequences\n",
    "        progress_bar = tqdm(sequence_groups, desc=\"Processing sequences\") if verbose else sequence_groups\n",
    "        \n",
    "        for seq_id_group in progress_bar:\n",
    "            if hasattr(train_sequences, 'group_by'):\n",
    "                seq_id, sequence = seq_id_group\n",
    "            else:\n",
    "                seq_id, sequence = seq_id_group\n",
    "            \n",
    "            try:\n",
    "                # Get sequence info\n",
    "                if hasattr(sequence, 'to_pandas'):\n",
    "                    seq_df = sequence.to_pandas()\n",
    "                else:\n",
    "                    seq_df = sequence\n",
    "                \n",
    "                subject_id = seq_df['subject'].iloc[0]\n",
    "                gesture = seq_df['gesture'].iloc[0]\n",
    "                \n",
    "                # Get demographics\n",
    "                if hasattr(train_demographics, 'filter'):\n",
    "                    demographics = train_demographics.filter(pl.col('subject') == subject_id)\n",
    "                    demo_values = self._extract_demographics(demographics)\n",
    "                else:\n",
    "                    demographics = train_demographics[train_demographics['subject'] == subject_id]\n",
    "                    demo_values = self._extract_demographics(demographics)\n",
    "                \n",
    "                # Extract kinematic features\n",
    "                features = extract_sequence_features(seq_df)\n",
    "                if features is None:\n",
    "                    failed_sequences += 1\n",
    "                    continue\n",
    "                \n",
    "                angular_velocity = features['angular_velocity']\n",
    "                \n",
    "                # Filter by sequence length\n",
    "                seq_len = len(angular_velocity)\n",
    "                if seq_len < CONFIG['MIN_SEQUENCE_LENGTH'] or seq_len > CONFIG['MAX_SEQUENCE_LENGTH']:\n",
    "                    failed_sequences += 1\n",
    "                    continue\n",
    "                \n",
    "                # Create training sample\n",
    "                sample = {\n",
    "                    'sequence_id': seq_id,\n",
    "                    'gesture': gesture,\n",
    "                    'gesture_idx': self.class_to_idx[gesture],\n",
    "                    'demographics': demo_values,\n",
    "                    'angular_velocity': angular_velocity,\n",
    "                    'sequence_length': seq_len,\n",
    "                    'subject_id': subject_id\n",
    "                }\n",
    "                \n",
    "                training_samples.append(sample)\n",
    "                \n",
    "            except Exception as e:\n",
    "                failed_sequences += 1\n",
    "                if verbose and failed_sequences <= 5:\n",
    "                    print(f\"   Warning: Failed to process sequence {seq_id}: {e}\")\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"✅ Prepared {len(training_samples)} samples\")\n",
    "            print(f\"   Failed sequences: {failed_sequences}\")\n",
    "            \n",
    "            # Show gesture distribution\n",
    "            gesture_counts = {}\n",
    "            for sample in training_samples:\n",
    "                gesture_counts[sample['gesture']] = gesture_counts.get(sample['gesture'], 0) + 1\n",
    "            \n",
    "            print(\"   Gesture distribution:\")\n",
    "            for gesture, count in sorted(gesture_counts.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "                print(f\"     {gesture}: {count}\")\n",
    "        \n",
    "        # Fit scalers\n",
    "        if training_samples:\n",
    "            all_angular_velocities = np.vstack([s['angular_velocity'] for s in training_samples])\n",
    "            all_demographics = np.array([s['demographics'] for s in training_samples])\n",
    "            \n",
    "            self.angular_velocity_scaler.fit(all_angular_velocities)\n",
    "            self.demographics_scaler.fit(all_demographics)\n",
    "        \n",
    "        return training_samples\n",
    "    \n",
    "    def train(self, training_samples, epochs=100, batch_size=32, lr=1e-3, \n",
    "              validation_split=0.2, patience=20, verbose=True):\n",
    "        \"\"\"Train the GNN kinematic model\"\"\"\n",
    "        if verbose:\n",
    "            print(f\"Training GNN kinematic model...\")\n",
    "            print(f\"   Samples: {len(training_samples)}\")\n",
    "            print(f\"   Epochs: {epochs}, Batch size: {batch_size}\")\n",
    "            print(f\"   Learning rate: {lr}\")\n",
    "        \n",
    "        # Split data\n",
    "        n_val = int(len(training_samples) * validation_split)\n",
    "        n_train = len(training_samples) - n_val\n",
    "        \n",
    "        indices = np.random.permutation(len(training_samples))\n",
    "        train_samples = [training_samples[i] for i in indices[:n_train]]\n",
    "        val_samples = [training_samples[i] for i in indices[n_train:]]\n",
    "        \n",
    "        # Setup training\n",
    "        optimizer = AdamW(self.kinematic_model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=patience//2)\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(epochs):\n",
    "            # Training phase\n",
    "            self.kinematic_model.train()\n",
    "            train_losses = []\n",
    "            \n",
    "            # Process in batches\n",
    "            for i in range(0, len(train_samples), batch_size):\n",
    "                batch = train_samples[i:i+batch_size]\n",
    "                loss = self._train_batch(batch, optimizer)\n",
    "                train_losses.append(loss)\n",
    "            \n",
    "            avg_train_loss = np.mean(train_losses)\n",
    "            \n",
    "            # Validation phase\n",
    "            if val_samples:\n",
    "                self.kinematic_model.eval()\n",
    "                val_losses = []\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    for i in range(0, len(val_samples), batch_size):\n",
    "                        batch = val_samples[i:i+batch_size]\n",
    "                        loss = self._validate_batch(batch)\n",
    "                        val_losses.append(loss)\n",
    "                \n",
    "                avg_val_loss = np.mean(val_losses)\n",
    "                \n",
    "                # Learning rate scheduling\n",
    "                scheduler.step(avg_val_loss)\n",
    "                \n",
    "                # Early stopping\n",
    "                if avg_val_loss < best_val_loss:\n",
    "                    best_val_loss = avg_val_loss\n",
    "                    patience_counter = 0\n",
    "                    # Save best model\n",
    "                    torch.save(self.kinematic_model.state_dict(), \n",
    "                             OUTPUT_DIR / 'best_gnn_model.pth')\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                \n",
    "                if patience_counter >= patience:\n",
    "                    if verbose:\n",
    "                        print(f\"Early stopping at epoch {epoch}\")\n",
    "                    break\n",
    "            else:\n",
    "                avg_val_loss = avg_train_loss\n",
    "            \n",
    "            # Record history\n",
    "            self.training_history['train_loss'].append(avg_train_loss)\n",
    "            self.training_history['val_loss'].append(avg_val_loss)\n",
    "            self.training_history['learning_rate'].append(optimizer.param_groups[0]['lr'])\n",
    "            \n",
    "            # Progress reporting\n",
    "            if verbose and (epoch % 10 == 0 or epoch == epochs - 1):\n",
    "                print(f\"Epoch {epoch:3d}: Train Loss = {avg_train_loss:.4f}, \"\n",
    "                      f\"Val Loss = {avg_val_loss:.4f}, LR = {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"✅ Training complete! Best validation loss: {best_val_loss:.4f}\")\n",
    "        \n",
    "        # Load best model\n",
    "        if (OUTPUT_DIR / 'best_gnn_model.pth').exists():\n",
    "            self.kinematic_model.load_state_dict(\n",
    "                torch.load(OUTPUT_DIR / 'best_gnn_model.pth', map_location=self.device)\n",
    "            )\n",
    "    \n",
    "    def _train_batch(self, batch, optimizer):\n",
    "        \"\"\"Train on a single batch\"\"\"\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Prepare batch data\n",
    "        gesture_indices = torch.tensor([s['gesture_idx'] for s in batch], \n",
    "                                      dtype=torch.long, device=self.device)\n",
    "        demographics = torch.tensor([s['demographics'] for s in batch], \n",
    "                                   dtype=torch.float32, device=self.device)\n",
    "        demographics = torch.tensor(self.demographics_scaler.transform(demographics.cpu().numpy()), \n",
    "                                   dtype=torch.float32, device=self.device)\n",
    "        \n",
    "        # Get maximum sequence length in batch\n",
    "        max_seq_len = max(s['sequence_length'] for s in batch)\n",
    "        \n",
    "        # Target angular velocities (padded)\n",
    "        target_angular_vels = []\n",
    "        for sample in batch:\n",
    "            ang_vel = sample['angular_velocity']\n",
    "            ang_vel_scaled = self.angular_velocity_scaler.transform(ang_vel)\n",
    "            \n",
    "            # Pad or truncate to max length\n",
    "            if len(ang_vel_scaled) < max_seq_len:\n",
    "                padding = np.zeros((max_seq_len - len(ang_vel_scaled), 3))\n",
    "                ang_vel_scaled = np.vstack([ang_vel_scaled, padding])\n",
    "            else:\n",
    "                ang_vel_scaled = ang_vel_scaled[:max_seq_len]\n",
    "            \n",
    "            target_angular_vels.append(ang_vel_scaled)\n",
    "        \n",
    "        target_angular_vels = torch.tensor(np.array(target_angular_vels), \n",
    "                                          dtype=torch.float32, device=self.device)\n",
    "        \n",
    "        # Generate predictions\n",
    "        predicted_angular_vels = self.kinematic_model(\n",
    "            gesture_indices, demographics, max_seq_len\n",
    "        )\n",
    "        \n",
    "        # Calculate loss (MSE + physics regularization)\n",
    "        mse_loss = F.mse_loss(predicted_angular_vels, target_angular_vels)\n",
    "        \n",
    "        # Physics regularization (encourage smooth trajectories)\n",
    "        velocity_diff = torch.diff(predicted_angular_vels, dim=1)\n",
    "        smoothness_loss = torch.mean(velocity_diff ** 2)\n",
    "        \n",
    "        total_loss = mse_loss + 0.01 * smoothness_loss\n",
    "        \n",
    "        # Backward pass\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.kinematic_model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        return total_loss.item()\n",
    "    \n",
    "    def _validate_batch(self, batch):\n",
    "        \"\"\"Validate on a single batch\"\"\"\n",
    "        # Similar to train batch but without gradients\n",
    "        gesture_indices = torch.tensor([s['gesture_idx'] for s in batch], \n",
    "                                      dtype=torch.long, device=self.device)\n",
    "        demographics = torch.tensor([s['demographics'] for s in batch], \n",
    "                                   dtype=torch.float32, device=self.device)\n",
    "        demographics = torch.tensor(self.demographics_scaler.transform(demographics.cpu().numpy()), \n",
    "                                   dtype=torch.float32, device=self.device)\n",
    "        \n",
    "        max_seq_len = max(s['sequence_length'] for s in batch)\n",
    "        \n",
    "        target_angular_vels = []\n",
    "        for sample in batch:\n",
    "            ang_vel = sample['angular_velocity']\n",
    "            ang_vel_scaled = self.angular_velocity_scaler.transform(ang_vel)\n",
    "            \n",
    "            if len(ang_vel_scaled) < max_seq_len:\n",
    "                padding = np.zeros((max_seq_len - len(ang_vel_scaled), 3))\n",
    "                ang_vel_scaled = np.vstack([ang_vel_scaled, padding])\n",
    "            else:\n",
    "                ang_vel_scaled = ang_vel_scaled[:max_seq_len]\n",
    "            \n",
    "            target_angular_vels.append(ang_vel_scaled)\n",
    "        \n",
    "        target_angular_vels = torch.tensor(np.array(target_angular_vels), \n",
    "                                          dtype=torch.float32, device=self.device)\n",
    "        \n",
    "        predicted_angular_vels = self.kinematic_model(\n",
    "            gesture_indices, demographics, max_seq_len\n",
    "        )\n",
    "        \n",
    "        mse_loss = F.mse_loss(predicted_angular_vels, target_angular_vels)\n",
    "        velocity_diff = torch.diff(predicted_angular_vels, dim=1)\n",
    "        smoothness_loss = torch.mean(velocity_diff ** 2)\n",
    "        \n",
    "        total_loss = mse_loss + 0.01 * smoothness_loss\n",
    "        return total_loss.item()\n",
    "    \n",
    "    def predict(self, test_sequence, test_demographics, top_k=1):\n",
    "        \"\"\"Predict gesture by comparing with all possible gestures\"\"\"\n",
    "        self.kinematic_model.eval()\n",
    "        \n",
    "        try:\n",
    "            # Extract actual angular velocity\n",
    "            features = extract_sequence_features(test_sequence)\n",
    "            if features is None:\n",
    "                return self.gesture_classes[0]  # Fallback\n",
    "            \n",
    "            actual_angular_vel = features['angular_velocity']\n",
    "            actual_angular_vel_scaled = self.angular_velocity_scaler.transform(actual_angular_vel)\n",
    "            \n",
    "            # Get demographics\n",
    "            demo_values = self._extract_demographics(test_demographics)\n",
    "            demo_values_scaled = self.demographics_scaler.transform([demo_values])[0]\n",
    "            \n",
    "            # Test against all gestures\n",
    "            similarities = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for gesture_name in self.gesture_classes:\n",
    "                    gesture_idx = torch.tensor([self.class_to_idx[gesture_name]], \n",
    "                                              dtype=torch.long, device=self.device)\n",
    "                    demographics_tensor = torch.tensor([demo_values_scaled], \n",
    "                                                       dtype=torch.float32, device=self.device)\n",
    "                    \n",
    "                    # Generate predicted angular velocity\n",
    "                    seq_len = len(actual_angular_vel)\n",
    "                    predicted_angular_vel = self.kinematic_model(\n",
    "                        gesture_idx, demographics_tensor, seq_len\n",
    "                    )\n",
    "                    \n",
    "                    # Calculate similarity\n",
    "                    predicted_np = predicted_angular_vel.cpu().numpy()[0]\n",
    "                    similarity = self._calculate_similarity(actual_angular_vel_scaled, predicted_np)\n",
    "                    similarities.append((gesture_name, similarity))\n",
    "            \n",
    "            # Sort by similarity\n",
    "            similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            if top_k == 1:\n",
    "                return similarities[0][0]\n",
    "            else:\n",
    "                return [s[0] for s in similarities[:top_k]]\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Prediction failed: {e}\")\n",
    "            return self.gesture_classes[0]  # Fallback\n",
    "    \n",
    "    def _calculate_similarity(self, actual, predicted):\n",
    "        \"\"\"Calculate similarity between actual and predicted angular velocities\"\"\"\n",
    "        try:\n",
    "            # Multiple similarity metrics\n",
    "            \n",
    "            # 1. Negative MSE (higher is better)\n",
    "            mse_similarity = -np.mean((actual - predicted) ** 2)\n",
    "            \n",
    "            # 2. Correlation coefficient\n",
    "            corr_similarity = 0\n",
    "            for dim in range(3):  # x, y, z components\n",
    "                if np.std(actual[:, dim]) > 1e-6 and np.std(predicted[:, dim]) > 1e-6:\n",
    "                    corr, _ = pearsonr(actual[:, dim], predicted[:, dim])\n",
    "                    corr_similarity += max(0, corr)  # Only positive correlations\n",
    "            corr_similarity /= 3\n",
    "            \n",
    "            # 3. DTW distance (for temporal alignment)\n",
    "            dtw_distance = 0\n",
    "            for dim in range(3):\n",
    "                distance, _ = fastdtw(actual[:, dim], predicted[:, dim], dist=euclidean)\n",
    "                dtw_distance += distance\n",
    "            dtw_similarity = -dtw_distance / 3\n",
    "            \n",
    "            # Combined similarity\n",
    "            total_similarity = (\n",
    "                0.4 * mse_similarity + \n",
    "                0.4 * corr_similarity +\n",
    "                0.2 * (dtw_similarity / 1000)  # Scale DTW\n",
    "            )\n",
    "            \n",
    "            return total_similarity\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Similarity calculation failed: {e}\")\n",
    "            return -float('inf')\n",
    "    \n",
    "    def _extract_demographics(self, demographics_data):\n",
    "        \"\"\"Extract demographics features\"\"\"\n",
    "        if demographics_data is None or len(demographics_data) == 0:\n",
    "            # Default demographics\n",
    "            return [1.0, 25.0, 1.0, 1.0, 170.0, 60.0, 25.0]\n",
    "        \n",
    "        try:\n",
    "            if hasattr(demographics_data, 'to_pandas'):\n",
    "                demo_df = demographics_data.to_pandas()\n",
    "            else:\n",
    "                demo_df = demographics_data\n",
    "            \n",
    "            if len(demo_df) == 0:\n",
    "                return [1.0, 25.0, 1.0, 1.0, 170.0, 60.0, 25.0]\n",
    "            \n",
    "            demo_values = []\n",
    "            for feature in DEMOGRAPHICS_FEATURES:\n",
    "                if feature in demo_df.columns:\n",
    "                    value = demo_df[feature].iloc[0]\n",
    "                    demo_values.append(float(value) if pd.notna(value) else 0.0)\n",
    "                else:\n",
    "                    demo_values.append(0.0)\n",
    "            \n",
    "            return demo_values\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Demographics extraction failed: {e}\")\n",
    "            return [1.0, 25.0, 1.0, 1.0, 170.0, 60.0, 25.0]\n",
    "    \n",
    "    def plot_training_history(self):\n",
    "        \"\"\"Plot training history\"\"\"\n",
    "        if not self.training_history['train_loss']:\n",
    "            print(\"No training history available\")\n",
    "            return\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        \n",
    "        # Loss plot\n",
    "        ax1.plot(self.training_history['train_loss'], label='Train Loss')\n",
    "        ax1.plot(self.training_history['val_loss'], label='Val Loss')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.set_title('Training History')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True)\n",
    "        \n",
    "        # Learning rate plot\n",
    "        ax2.plot(self.training_history['learning_rate'])\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Learning Rate')\n",
    "        ax2.set_title('Learning Rate Schedule')\n",
    "        ax2.set_yscale('log')\n",
    "        ax2.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "print(\"✅ GNN Gesture Classifier defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"Load training and test data\"\"\"\n",
    "    print(\"Loading data for GNN training...\")\n",
    "    \n",
    "    data = {}\n",
    "    \n",
    "    for key, path in DATA_PATHS.items():\n",
    "        if path.exists():\n",
    "            try:\n",
    "                df = pl.read_csv(str(path))\n",
    "                data[key] = df\n",
    "                print(f\"   ✓ {key}: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ✗ {key}: Failed to load - {e}\")\n",
    "                data[key] = None\n",
    "        else:\n",
    "            print(f\"   ⚠ {key}: File not found at {path}\")\n",
    "            data[key] = None\n",
    "    \n",
    "    return data\n",
    "\n",
    "def create_sample_data_for_gnn():\n",
    "    \"\"\"Create sample data optimized for GNN training\"\"\"\n",
    "    print(\"Creating sample data for GNN testing...\")\n",
    "    \n",
    "    # Create more realistic synthetic data\n",
    "    n_samples_per_seq = 50  # Shorter sequences for faster training\n",
    "    n_sequences = 200  # More sequences for better variety\n",
    "    \n",
    "    sample_data = []\n",
    "    \n",
    "    for seq_idx in range(n_sequences):\n",
    "        # Random gesture\n",
    "        gesture = np.random.choice(GESTURE_CLASSES)\n",
    "        subject_id = f'SUBJ_{seq_idx % 50:06d}'  # Reuse subjects\n",
    "        \n",
    "        # Generate gesture-specific patterns\n",
    "        t = np.linspace(0, 2, n_samples_per_seq)  # 2 seconds at 25Hz\n",
    "        \n",
    "        # Create gesture-specific motion patterns\n",
    "        if 'pull hair' in gesture:\n",
    "            # Sharp, quick movements\n",
    "            base_freq = 2.0\n",
    "            amplitude = 1.5\n",
    "        elif 'scratch' in gesture:\n",
    "            # Repetitive, oscillatory\n",
    "            base_freq = 3.0\n",
    "            amplitude = 1.0\n",
    "        elif 'Text on phone' in gesture:\n",
    "            # Small, precise movements\n",
    "            base_freq = 5.0\n",
    "            amplitude = 0.3\n",
    "        else:\n",
    "            # Default pattern\n",
    "            base_freq = 1.0\n",
    "            amplitude = 0.8\n",
    "        \n",
    "        # Generate quaternion-based rotation data\n",
    "        angle_x = amplitude * np.sin(2 * np.pi * base_freq * t) + 0.1 * np.random.randn(n_samples_per_seq)\n",
    "        angle_y = amplitude * np.cos(2 * np.pi * base_freq * t) + 0.1 * np.random.randn(n_samples_per_seq)\n",
    "        angle_z = 0.5 * amplitude * np.sin(4 * np.pi * base_freq * t) + 0.1 * np.random.randn(n_samples_per_seq)\n",
    "        \n",
    "        # Convert to quaternions\n",
    "        rot_w = np.cos(np.sqrt(angle_x**2 + angle_y**2 + angle_z**2) / 2)\n",
    "        rot_x = angle_x / (2 * np.sqrt(angle_x**2 + angle_y**2 + angle_z**2 + 1e-8))\n",
    "        rot_y = angle_y / (2 * np.sqrt(angle_x**2 + angle_y**2 + angle_z**2 + 1e-8))\n",
    "        rot_z = angle_z / (2 * np.sqrt(angle_x**2 + angle_y**2 + angle_z**2 + 1e-8))\n",
    "        \n",
    "        # Generate corresponding accelerometer data\n",
    "        acc_x = np.gradient(np.gradient(angle_x)) + np.random.randn(n_samples_per_seq) * 0.1\n",
    "        acc_y = np.gradient(np.gradient(angle_y)) + np.random.randn(n_samples_per_seq) * 0.1\n",
    "        acc_z = np.gradient(np.gradient(angle_z)) + 9.81 + np.random.randn(n_samples_per_seq) * 0.1\n",
    "        \n",
    "        for i in range(n_samples_per_seq):\n",
    "            sample_data.append({\n",
    "                'sequence_id': f'SEQ_{seq_idx:06d}',\n",
    "                'subject': subject_id,\n",
    "                'gesture': gesture,\n",
    "                'acc_x': acc_x[i],\n",
    "                'acc_y': acc_y[i],\n",
    "                'acc_z': acc_z[i],\n",
    "                'rot_w': rot_w[i],\n",
    "                'rot_x': rot_x[i],\n",
    "                'rot_y': rot_y[i],\n",
    "                'rot_z': rot_z[i],\n",
    "            })\n",
    "            \n",
    "            # Add thermal and TOF data (simplified)\n",
    "            for thm_idx in range(1, 6):\n",
    "                sample_data[-1][f'thm_{thm_idx}'] = np.random.uniform(25, 35)\n",
    "            \n",
    "            for tof_idx in range(1, 6):\n",
    "                for pixel in range(64):\n",
    "                    sample_data[-1][f'tof_{tof_idx}_v{pixel}'] = np.random.choice(\n",
    "                        [-1, np.random.uniform(0, 1000)], p=[0.1, 0.9]\n",
    "                    )\n",
    "    \n",
    "    train_data = pl.DataFrame(sample_data)\n",
    "    \n",
    "    # Create demographics\n",
    "    unique_subjects = train_data['subject'].unique().to_list()\n",
    "    demographics_data = []\n",
    "    \n",
    "    for subject in unique_subjects:\n",
    "        demographics_data.append({\n",
    "            'subject': subject,\n",
    "            'adult_child': np.random.choice([0, 1]),\n",
    "            'age': np.random.randint(8, 65),\n",
    "            'sex': np.random.choice([0, 1]),\n",
    "            'handedness': np.random.choice([0, 1]),\n",
    "            'height_cm': np.random.uniform(120, 190),\n",
    "            'shoulder_to_wrist_cm': np.random.uniform(50, 80),\n",
    "            'elbow_to_wrist_cm': np.random.uniform(20, 35)\n",
    "        })\n",
    "    \n",
    "    train_demographics = pl.DataFrame(demographics_data)\n",
    "    \n",
    "    # Create test set (smaller)\n",
    "    test_data = train_data.sample(500, seed=42)\n",
    "    test_demographics = train_demographics.sample(20, seed=42)\n",
    "    \n",
    "    print(f\"   ✓ Sample train data: {train_data.shape}\")\n",
    "    print(f\"   ✓ Sample train demographics: {train_demographics.shape}\")\n",
    "    print(f\"   ✓ Sample test data: {test_data.shape}\")\n",
    "    print(f\"   ✓ Sample test demographics: {test_demographics.shape}\")\n",
    "    \n",
    "    return {\n",
    "        'train_data': train_data,\n",
    "        'train_demographics': train_demographics,\n",
    "        'test_data': test_data,\n",
    "        'test_demographics': test_demographics\n",
    "    }\n",
    "\n",
    "# Load data\n",
    "data = load_data()\n",
    "\n",
    "# Use sample data if real data is not available\n",
    "if data['train_data'] is None:\n",
    "    print(\"\\nReal data not found, using sample data for GNN demonstration...\")\n",
    "    data = create_sample_data_for_gnn()\n",
    "\n",
    "print(f\"\\n✅ Data loading complete for GNN training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Initialization and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the virtual kinematic chain model\n",
    "kinematic_model = VirtualKinematicChain(\n",
    "    n_joints=KINEMATIC_CONFIG['n_joints'],\n",
    "    hidden_dim=KINEMATIC_CONFIG['hidden_dim'],\n",
    "    n_classes=len(GESTURE_CLASSES),\n",
    "    gnn_layers=KINEMATIC_CONFIG['gnn_layers'],\n",
    "    attention_heads=KINEMATIC_CONFIG['attention_heads'],\n",
    "    dropout=KINEMATIC_CONFIG['dropout']\n",
    ")\n",
    "\n",
    "# Initialize the GNN classifier\n",
    "gnn_classifier = GNNGestureClassifier(\n",
    "    kinematic_model=kinematic_model,\n",
    "    gesture_classes=GESTURE_CLASSES,\n",
    "    device=CONFIG['DEVICE']\n",
    ")\n",
    "\n",
    "print(f\"\\n📊 Model Architecture Summary:\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in kinematic_model.parameters()):,}\")\n",
    "print(f\"   Trainable: {sum(p.numel() for p in kinematic_model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# Visualize the kinematic graph\n",
    "print(\"\\n🔗 Kinematic Chain Visualization:\")\n",
    "kinematic_model.visualize_kinematic_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the GNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG['TRAIN_MODE'] and data['train_data'] is not None:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"TRAINING GNN KINEMATICS MODEL\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # Prepare training data\n",
    "        print(\"\\n1. Preparing training data...\")\n",
    "        training_samples = gnn_classifier.prepare_training_data(\n",
    "            data['train_data'],\n",
    "            data['train_demographics'], \n",
    "            sample_limit=100,  # Limit for demonstration\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        if len(training_samples) == 0:\n",
    "            raise ValueError(\"No valid training samples prepared\")\n",
    "        \n",
    "        # Train the model\n",
    "        print(\"\\n2. Training GNN model...\")\n",
    "        gnn_classifier.train(\n",
    "            training_samples,\n",
    "            epochs=CONFIG['N_EPOCHS'] // 2,  # Reduced for demo\n",
    "            batch_size=CONFIG['BATCH_SIZE'] // 2,\n",
    "            lr=CONFIG['LEARNING_RATE'],\n",
    "            validation_split=0.2,\n",
    "            patience=CONFIG['PATIENCE'],\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        # Save the model\n",
    "        print(\"\\n3. Saving model...\")\n",
    "        model_path = OUTPUT_DIR / \"gnn_kinematics_model.pth\"\n",
    "        torch.save({\n",
    "            'model_state_dict': kinematic_model.state_dict(),\n",
    "            'angular_velocity_scaler': gnn_classifier.angular_velocity_scaler,\n",
    "            'demographics_scaler': gnn_classifier.demographics_scaler,\n",
    "            'gesture_classes': GESTURE_CLASSES,\n",
    "            'config': CONFIG,\n",
    "            'kinematic_config': KINEMATIC_CONFIG\n",
    "        }, model_path)\n",
    "        print(f\"Model saved to {model_path}\")\n",
    "        \n",
    "        # Plot training history\n",
    "        print(\"\\n4. Training history:\")\n",
    "        gnn_classifier.plot_training_history()\n",
    "        \n",
    "        training_success = True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ GNN training failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        training_success = False\n",
    "        \n",
    "else:\n",
    "    print(\"Skipping GNN training (TRAIN_MODE=False or no training data)\")\n",
    "    training_success = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_gnn_model(classifier, test_data, test_demographics, n_samples=10):\n",
    "    \"\"\"Evaluate the trained GNN model\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"GNN MODEL EVALUATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if test_data is None:\n",
    "        print(\"❌ No test data available\")\n",
    "        return\n",
    "    \n",
    "    # Get sample sequences\n",
    "    sequence_groups = list(test_data.group_by('sequence_id'))\n",
    "    sample_sequences = sequence_groups[:min(n_samples, len(sequence_groups))]\n",
    "    \n",
    "    print(f\"\\nEvaluating on {len(sample_sequences)} test sequences...\\n\")\n",
    "    \n",
    "    predictions = []\n",
    "    actual_labels = []\n",
    "    prediction_times = []\n",
    "    similarities_data = []\n",
    "    \n",
    "    for i, (seq_id, sequence) in enumerate(sample_sequences):\n",
    "        try:\n",
    "            # Get actual label\n",
    "            actual_gesture = sequence['gesture'][0] if 'gesture' in sequence.columns else \"Unknown\"\n",
    "            actual_labels.append(actual_gesture)\n",
    "            \n",
    "            # Get demographics\n",
    "            subject_id = sequence['subject'][0]\n",
    "            demographics = test_demographics.filter(pl.col('subject') == subject_id)\n",
    "            \n",
    "            # Make prediction with timing\n",
    "            import time\n",
    "            start_time = time.time()\n",
    "            predicted_gesture = classifier.predict(sequence, demographics)\n",
    "            prediction_time = time.time() - start_time\n",
    "            \n",
    "            predictions.append(predicted_gesture)\n",
    "            prediction_times.append(prediction_time)\n",
    "            \n",
    "            # Show result\n",
    "            status = \"✓\" if predicted_gesture == actual_gesture else \"✗\"\n",
    "            print(f\"{i+1:2d}. {status} Actual: {actual_gesture:<25} | \"\n",
    "                  f\"Predicted: {predicted_gesture:<25} | Time: {prediction_time:.3f}s\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"{i+1:2d}. ❌ Error processing sequence {seq_id}: {e}\")\n",
    "            predictions.append(\"Error\")\n",
    "            actual_labels.append(\"Error\")\n",
    "            prediction_times.append(0)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    if len(predictions) > 0:\n",
    "        valid_predictions = [(a, p) for a, p in zip(actual_labels, predictions) \n",
    "                           if a != \"Error\" and p != \"Error\" and a != \"Unknown\"]\n",
    "        \n",
    "        if valid_predictions:\n",
    "            valid_actual, valid_pred = zip(*valid_predictions)\n",
    "            accuracy = accuracy_score(valid_actual, valid_pred)\n",
    "            \n",
    "            print(f\"\\n📊 GNN EVALUATION RESULTS:\")\n",
    "            print(f\"   Accuracy: {accuracy:.2%} ({len(valid_predictions)} valid predictions)\")\n",
    "            print(f\"   Average prediction time: {np.mean(prediction_times):.3f}s\")\n",
    "            print(f\"   Error rate: {predictions.count('Error')/len(predictions):.1%}\")\n",
    "            \n",
    "            # Show prediction distribution\n",
    "            pred_counts = pd.Series(predictions).value_counts()\n",
    "            print(f\"\\n🎯 PREDICTION DISTRIBUTION:\")\n",
    "            for pred, count in pred_counts.head(5).items():\n",
    "                print(f\"   {pred}: {count}\")\n",
    "        else:\n",
    "            print(\"\\n❌ No valid predictions to evaluate\")\n",
    "    \n",
    "    return predictions, actual_labels, prediction_times\n",
    "\n",
    "# Run evaluation\n",
    "if training_success and data['test_data'] is not None:\n",
    "    evaluation_results = evaluate_gnn_model(\n",
    "        gnn_classifier,\n",
    "        data['test_data'],\n",
    "        data['test_demographics'],\n",
    "        n_samples=6  # Reduced for demo (GNN inference is slower)\n",
    "    )\n",
    "else:\n",
    "    print(\"Skipping GNN evaluation (no trained model or test data)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gesture Generation Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_gesture_generation(classifier, gesture_name, demographics_sample=None, seq_length=50):\n",
    "    \"\"\"Visualize generated angular velocity patterns for a specific gesture\"\"\"\n",
    "    if not training_success:\n",
    "        print(\"Model not trained, skipping visualization\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n🎨 Generating motion pattern for: {gesture_name}\")\n",
    "    \n",
    "    try:\n",
    "        # Use sample demographics if not provided\n",
    "        if demographics_sample is None:\n",
    "            demographics_sample = [1.0, 30.0, 1.0, 1.0, 175.0, 65.0, 28.0]  # Adult male\n",
    "        \n",
    "        # Prepare input\n",
    "        gesture_idx = torch.tensor([gnn_classifier.class_to_idx[gesture_name]], \n",
    "                                  dtype=torch.long, device=CONFIG['DEVICE'])\n",
    "        demographics_scaled = gnn_classifier.demographics_scaler.transform([demographics_sample])\n",
    "        demographics_tensor = torch.tensor(demographics_scaled, dtype=torch.float32, device=CONFIG['DEVICE'])\n",
    "        \n",
    "        # Generate pattern\n",
    "        gnn_classifier.kinematic_model.eval()\n",
    "        with torch.no_grad():\n",
    "            generated_pattern, intermediates = gnn_classifier.kinematic_model(\n",
    "                gesture_idx, demographics_tensor, seq_length, return_intermediates=True\n",
    "            )\n",
    "        \n",
    "        # Convert to numpy\n",
    "        pattern_np = generated_pattern.cpu().numpy()[0]  # First (and only) sample\n",
    "        \n",
    "        # Visualize\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Angular velocity components\n",
    "        time_steps = np.arange(seq_length) / CONFIG['SAMPLE_RATE']\n",
    "        \n",
    "        axes[0, 0].plot(time_steps, pattern_np[:, 0], 'r-', linewidth=2, label='X-axis')\n",
    "        axes[0, 0].plot(time_steps, pattern_np[:, 1], 'g-', linewidth=2, label='Y-axis')\n",
    "        axes[0, 0].plot(time_steps, pattern_np[:, 2], 'b-', linewidth=2, label='Z-axis')\n",
    "        axes[0, 0].set_title(f'Generated Angular Velocity Pattern\\n{gesture_name}')\n",
    "        axes[0, 0].set_xlabel('Time (s)')\n",
    "        axes[0, 0].set_ylabel('Angular Velocity (rad/s)')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True)\n",
    "        \n",
    "        # Magnitude over time\n",
    "        magnitude = np.linalg.norm(pattern_np, axis=1)\n",
    "        axes[0, 1].plot(time_steps, magnitude, 'purple', linewidth=2)\n",
    "        axes[0, 1].set_title('Angular Velocity Magnitude')\n",
    "        axes[0, 1].set_xlabel('Time (s)')\n",
    "        axes[0, 1].set_ylabel('Magnitude (rad/s)')\n",
    "        axes[0, 1].grid(True)\n",
    "        \n",
    "        # 3D trajectory visualization\n",
    "        ax_3d = fig.add_subplot(2, 2, 3, projection='3d')\n",
    "        ax_3d.plot(pattern_np[:, 0], pattern_np[:, 1], pattern_np[:, 2], 'b-', linewidth=2)\n",
    "        ax_3d.scatter(pattern_np[0, 0], pattern_np[0, 1], pattern_np[0, 2], \n",
    "                     c='green', s=100, label='Start')\n",
    "        ax_3d.scatter(pattern_np[-1, 0], pattern_np[-1, 1], pattern_np[-1, 2], \n",
    "                     c='red', s=100, label='End')\n",
    "        ax_3d.set_title('3D Angular Velocity Trajectory')\n",
    "        ax_3d.set_xlabel('X (rad/s)')\n",
    "        ax_3d.set_ylabel('Y (rad/s)')\n",
    "        ax_3d.set_zlabel('Z (rad/s)')\n",
    "        ax_3d.legend()\n",
    "        \n",
    "        # Frequency analysis\n",
    "        from scipy.fft import fft, fftfreq\n",
    "        fft_vals = fft(magnitude)\n",
    "        freqs = fftfreq(len(magnitude), 1/CONFIG['SAMPLE_RATE'])\n",
    "        \n",
    "        # Only plot positive frequencies\n",
    "        pos_freqs = freqs[:len(freqs)//2]\n",
    "        pos_fft = np.abs(fft_vals[:len(fft_vals)//2])\n",
    "        \n",
    "        axes[1, 1].plot(pos_freqs, pos_fft, 'orange', linewidth=2)\n",
    "        axes[1, 1].set_title('Frequency Spectrum')\n",
    "        axes[1, 1].set_xlabel('Frequency (Hz)')\n",
    "        axes[1, 1].set_ylabel('Amplitude')\n",
    "        axes[1, 1].grid(True)\n",
    "        axes[1, 1].set_xlim(0, 10)  # Focus on low frequencies\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print statistics\n",
    "        print(f\"\\n📈 Generated Pattern Statistics:\")\n",
    "        print(f\"   Duration: {seq_length / CONFIG['SAMPLE_RATE']:.2f} seconds\")\n",
    "        print(f\"   Max magnitude: {np.max(magnitude):.3f} rad/s\")\n",
    "        print(f\"   Mean magnitude: {np.mean(magnitude):.3f} rad/s\")\n",
    "        print(f\"   Dominant frequency: {pos_freqs[np.argmax(pos_fft)]:.2f} Hz\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Visualization failed: {e}\")\n",
    "\n",
    "# Visualize gesture generation for different gestures\n",
    "if training_success:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"GESTURE GENERATION VISUALIZATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Visualize a few different gestures\n",
    "    sample_gestures = ['Text on phone', 'Above ear - pull hair', 'Wave hello']\n",
    "    \n",
    "    for gesture in sample_gestures:\n",
    "        if gesture in GESTURE_CLASSES:\n",
    "            visualize_gesture_generation(gnn_classifier, gesture, seq_length=100)\n",
    "else:\n",
    "    print(\"Gesture generation visualization requires trained model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration with Existing Prediction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhanced_predict_gnn(sequence, demographics, gnn_model=None, \n",
    "                        original_predict_func=None, use_ensemble=True):\n",
    "    \"\"\"Enhanced prediction function using GNN kinematic model\"\"\"\n",
    "    \n",
    "    predictions = {}\n",
    "    confidences = {}\n",
    "    \n",
    "    # GNN kinematic prediction\n",
    "    if gnn_model is not None:\n",
    "        try:\n",
    "            gnn_pred = gnn_model.predict(sequence, demographics, top_k=3)\n",
    "            if isinstance(gnn_pred, list):\n",
    "                predictions['gnn'] = gnn_pred[0]  # Top prediction\n",
    "                confidences['gnn'] = 1.0 / (1 + len(gnn_pred))  # Simple confidence based on ranking\n",
    "            else:\n",
    "                predictions['gnn'] = gnn_pred\n",
    "                confidences['gnn'] = 0.8  # Default confidence\n",
    "        except Exception as e:\n",
    "            print(f\"GNN prediction failed: {e}\")\n",
    "            confidences['gnn'] = 0.0\n",
    "    \n",
    "    # Original model prediction (placeholder)\n",
    "    if original_predict_func is not None:\n",
    "        try:\n",
    "            original_pred = original_predict_func(sequence, demographics)\n",
    "            predictions['original'] = original_pred\n",
    "            confidences['original'] = 0.9  # Typically higher confidence for trained models\n",
    "        except Exception as e:\n",
    "            print(f\"Original prediction failed: {e}\")\n",
    "            confidences['original'] = 0.0\n",
    "    \n",
    "    # Ensemble decision\n",
    "    if len(predictions) == 0:\n",
    "        return GESTURE_CLASSES[0]  # Default fallback\n",
    "    elif len(predictions) == 1:\n",
    "        return list(predictions.values())[0]\n",
    "    elif use_ensemble:\n",
    "        # Weighted ensemble based on confidence\n",
    "        total_confidence = sum(confidences.values())\n",
    "        if total_confidence > 0:\n",
    "            # For simplicity, return prediction with highest confidence\n",
    "            best_model = max(confidences.items(), key=lambda x: x[1])[0]\n",
    "            return predictions[best_model]\n",
    "        else:\n",
    "            return list(predictions.values())[0]\n",
    "    else:\n",
    "        # Return GNN prediction if available, otherwise first available\n",
    "        return predictions.get('gnn', list(predictions.values())[0])\n",
    "\n",
    "# Integration example\n",
    "if training_success:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"GNN PREDICTION PIPELINE INTEGRATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(\"\\n✅ Enhanced GNN prediction pipeline ready\")\n",
    "    print(\"\\nUsage example:\")\n",
    "    print(\"```python\")\n",
    "    print(\"# For single prediction with GNN\")\n",
    "    print(\"predicted_gesture = enhanced_predict_gnn(\")\n",
    "    print(\"    sequence=test_sequence,\")\n",
    "    print(\"    demographics=test_demographics,\")\n",
    "    print(\"    gnn_model=gnn_classifier\")\n",
    "    print(\")\")\n",
    "    print(\"```\")\n",
    "    \n",
    "    # Integration with Kaggle evaluation server\n",
    "    print(\"\\n🔗 Integration with evaluation server:\")\n",
    "    print(\"```python\")\n",
    "    print(\"def predict(sequence, demographics):\")\n",
    "    print(\"    return enhanced_predict_gnn(\")\n",
    "    print(\"        sequence, demographics, gnn_classifier\")\n",
    "    print(\"    )\")\n",
    "    print(\"\")\n",
    "    print(\"# Use with existing evaluation framework\")\n",
    "    print(\"# inference_server = CMIInferenceServer(predict)\")\n",
    "    print(\"```\")\n",
    "    \n",
    "    # Show model comparison\n",
    "    print(\"\\n🔬 GNN Model Characteristics:\")\n",
    "    print(\"   ✓ Physics-informed: Uses kinematic constraints\")\n",
    "    print(\"   ✓ Interpretable: Generates expected motion patterns\")\n",
    "    print(\"   ✓ Individual-aware: Adapts to body measurements\")\n",
    "    print(\"   ✓ Novel approach: Generation + comparison vs direct classification\")\n",
    "    print(\"   ⚠ Computationally intensive: Requires testing all gestures\")\n",
    "    print(\"   ⚠ Complex training: More parameters and longer convergence\")\n",
    "else:\n",
    "    print(\"GNN integration available after successful training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"GNN KINEMATICS MODEL - IMPLEMENTATION SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n🎯 NOVEL APPROACH OVERVIEW:\")\n",
    "print(\"   • Virtual kinematic chain: shoulder → elbow → wrist\")\n",
    "print(\"   • Gesture-specific motion generation using GNN\")\n",
    "print(\"   • Comparison-based classification (generate + compare)\")\n",
    "print(\"   • Physics-informed constraints and demographics adaptation\")\n",
    "\n",
    "print(\"\\n✅ IMPLEMENTED COMPONENTS:\")\n",
    "print(\"   ✓ VirtualKinematicChain - GNN model with GAT layers\")\n",
    "print(\"   ✓ GNNGestureClassifier - Training and inference pipeline\")\n",
    "print(\"   ✓ Physics constraints - Smooth trajectory regularization\")\n",
    "print(\"   ✓ Demographics integration - Body-aware motion generation\")\n",
    "print(\"   ✓ Multi-metric similarity - MSE + Correlation + DTW\")\n",
    "print(\"   ✓ Visualization tools - Motion pattern analysis\")\n",
    "\n",
    "print(\"\\n🔧 TECHNICAL INNOVATIONS:\")\n",
    "print(\"   • Graph Attention Networks for joint relationships\")\n",
    "print(\"   • LSTM temporal modeling for sequence consistency\")\n",
    "print(\"   • Physics-informed loss functions\")\n",
    "print(\"   • Multi-scale similarity comparison (time + frequency domain)\")\n",
    "print(\"   • Demographics-conditioned motion generation\")\n",
    "\n",
    "if training_success:\n",
    "    print(\"\\n🎉 TRAINING STATUS: ✅ SUCCESSFUL\")\n",
    "    print(f\"   Model saved to: {OUTPUT_DIR / 'gnn_kinematics_model.pth'}\")\n",
    "    print(\"   Ready for production testing\")\n",
    "    \n",
    "    # Show training statistics\n",
    "    if gnn_classifier.training_history['train_loss']:\n",
    "        final_train_loss = gnn_classifier.training_history['train_loss'][-1]\n",
    "        final_val_loss = gnn_classifier.training_history['val_loss'][-1]\n",
    "        print(f\"   Final training loss: {final_train_loss:.4f}\")\n",
    "        print(f\"   Final validation loss: {final_val_loss:.4f}\")\nelse:\n",
    "    print(\"\\n⚠️  TRAINING STATUS: ❌ REQUIRES SETUP\")\n",
    "    print(\"   Need real sensor data and proper hyperparameter tuning\")\n",
    "\n",
    "print(\"\\n📋 NEXT STEPS FOR PRODUCTION:\")\n",
    "print(\"   1. Scale up training with full dataset\")\n",
    "print(\"   2. Optimize GNN architecture (layer depth, attention heads)\")\n",
    "print(\"   3. Improve physics constraints based on biomechanics literature\")\n",
    "print(\"   4. Implement efficient batch inference for real-time use\")\n",
    "print(\"   5. Compare performance against hybrid LightGBM approach\")\n",
    "print(\"   6. Fine-tune similarity metrics and ensemble weights\")\n",
    "\n",
    "print(\"\\n💡 RESEARCH IMPLICATIONS:\")\n",
    "print(\"   • Novel generative approach to gesture classification\")\n",
    "print(\"   • Physics-informed neural networks for human motion\")\n",
    "print(\"   • Interpretable AI through motion pattern generation\")\n",
    "print(\"   • Individual adaptation in wearable sensor applications\")\n",
    "\n",
    "print(\"\\n⚖️  TRADE-OFFS:\")\n",
    "print(\"   Pros: Novel, interpretable, physics-informed, individual-aware\")\n",
    "print(\"   Cons: Complex, computationally intensive, requires more tuning\")\n",
    "print(\"   Best for: Research applications, interpretability requirements\")\n",
    "print(\"   Consider hybrid approach for: Production deployment, speed requirements\")\n",
    "\n",
    "print(\"\\n🏁 GNN Kinematics implementation complete!\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}